{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74148cee",
   "metadata": {},
   "source": [
    "# Code for reproducing the experiment of \"Exploring the use of Language Models to Enhance Datasets Explainability via Documentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00583d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "java version \"20.0.1\" 2023-04-18\n",
      "Java(TM) SE Runtime Environment (build 20.0.1+9-29)\n",
      "Java HotSpot(TM) 64-Bit Server VM (build 20.0.1+9-29, mixed mode, sharing)\n",
      "Requirement already satisfied: absl-py==1.3.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: aiofiles==23.1.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: aiohttp==3.8.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (3.8.4)\n",
      "Requirement already satisfied: aiosignal==1.3.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: alembic==1.8.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (1.8.1)\n",
      "Requirement already satisfied: altair==5.0.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (5.0.0)\n",
      "Requirement already satisfied: anyio==3.6.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (3.6.2)\n",
      "Requirement already satisfied: appdirs==1.4.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.4.4)\n",
      "Requirement already satisfied: appnope==0.1.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.1.3)\n",
      "Requirement already satisfied: asttokens==2.1.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (2.1.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (1.6.3)\n",
      "Requirement already satisfied: async-timeout==4.0.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (4.0.2)\n",
      "Requirement already satisfied: attrs==23.1.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 13)) (23.1.0)\n",
      "Requirement already satisfied: azure-ai-formrecognizer==3.2.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (3.2.0)\n",
      "Requirement already satisfied: azure-common==1.1.28 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 15)) (1.1.28)\n",
      "Requirement already satisfied: azure-core==1.26.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (1.26.1)\n",
      "Requirement already satisfied: backcall==0.2.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (0.2.0)\n",
      "Requirement already satisfied: backoff==2.2.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 18)) (2.2.1)\n",
      "Requirement already satisfied: beautifulsoup4==4.11.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (4.11.1)\n",
      "Requirement already satisfied: blobfile==2.0.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (2.0.0)\n",
      "Requirement already satisfied: cachetools==5.2.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (5.2.0)\n",
      "Requirement already satisfied: camelot-py==0.9.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (0.9.0)\n",
      "Requirement already satisfied: certifi==2023.5.7 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (2023.5.7)\n",
      "Requirement already satisfied: cffi==1.15.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (1.15.1)\n",
      "Requirement already satisfied: chardet==5.1.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 25)) (5.1.0)\n",
      "Requirement already satisfied: charset-normalizer==3.1.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 26)) (3.1.0)\n",
      "Requirement already satisfied: click==8.1.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 27)) (8.1.3)\n",
      "Requirement already satisfied: cloudpickle==2.2.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 28)) (2.2.0)\n",
      "Requirement already satisfied: cohere==4.1.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 29)) (4.1.3)\n",
      "Requirement already satisfied: construct==2.5.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 30)) (2.5.3)\n",
      "Requirement already satisfied: contourpy==1.0.7 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 31)) (1.0.7)\n",
      "Requirement already satisfied: cryptography==40.0.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 32)) (40.0.2)\n",
      "Requirement already satisfied: cycler==0.11.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 33)) (0.11.0)\n",
      "Requirement already satisfied: databricks-cli==0.17.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 34)) (0.17.3)\n",
      "Requirement already satisfied: dataclasses-json==0.5.7 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 35)) (0.5.7)\n",
      "Requirement already satisfied: datasets==2.7.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 36)) (2.7.1)\n",
      "Requirement already satisfied: debugpy==1.6.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 37)) (1.6.3)\n",
      "Requirement already satisfied: decorator==5.1.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 38)) (5.1.1)\n",
      "Requirement already satisfied: dill==0.3.6 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 39)) (0.3.6)\n",
      "Requirement already satisfied: Distance==0.1.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 40)) (0.1.3)\n",
      "Requirement already satisfied: distlib==0.3.6 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 41)) (0.3.6)\n",
      "Requirement already satisfied: distro==1.8.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 42)) (1.8.0)\n",
      "Requirement already satisfied: docker==6.0.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 43)) (6.0.1)\n",
      "Requirement already satisfied: docopt==0.6.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 44)) (0.6.2)\n",
      "Requirement already satisfied: elasticsearch==7.17.7 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 45)) (7.17.7)\n",
      "Requirement already satisfied: entrypoints==0.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 46)) (0.4)\n",
      "Requirement already satisfied: et-xmlfile==1.1.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 47)) (1.1.0)\n",
      "Requirement already satisfied: executing==1.2.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 48)) (1.2.0)\n",
      "Requirement already satisfied: extruct==0.14.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 49)) (0.14.0)\n",
      "Requirement already satisfied: faiss-cpu==1.7.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 50)) (1.7.2)\n",
      "Requirement already satisfied: farm-haystack==1.12.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 51)) (1.12.2)\n",
      "Requirement already satisfied: fastapi==0.95.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 52)) (0.95.2)\n",
      "Requirement already satisfied: fastjsonschema==2.16.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 53)) (2.16.2)\n",
      "Requirement already satisfied: ffmpy==0.3.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 54)) (0.3.0)\n",
      "Requirement already satisfied: filelock==3.12.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 55)) (3.12.0)\n",
      "Requirement already satisfied: Flask==2.2.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 56)) (2.2.2)\n",
      "Requirement already satisfied: flatbuffers==22.10.26 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 57)) (22.10.26)\n",
      "Requirement already satisfied: fonttools==4.39.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 58)) (4.39.4)\n",
      "Requirement already satisfied: frozenlist==1.3.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 59)) (1.3.3)\n",
      "Requirement already satisfied: fsspec==2023.5.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 60)) (2023.5.0)\n",
      "Requirement already satisfied: future==0.18.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 61)) (0.18.2)\n",
      "Requirement already satisfied: gast==0.4.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 62)) (0.4.0)\n",
      "Requirement already satisfied: gitdb==4.0.9 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 63)) (4.0.9)\n",
      "Requirement already satisfied: GitPython==3.1.29 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 64)) (3.1.29)\n",
      "Requirement already satisfied: google-auth==2.14.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 65)) (2.14.1)\n",
      "Requirement already satisfied: google-auth-oauthlib==0.4.6 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 66)) (0.4.6)\n",
      "Requirement already satisfied: google-pasta==0.2.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 67)) (0.2.0)\n",
      "Requirement already satisfied: gpt-index==0.5.12 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 68)) (0.5.12)\n",
      "Requirement already satisfied: gradio==3.26.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 69)) (3.26.0)\n",
      "Requirement already satisfied: gradio_client==0.1.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 70)) (0.1.2)\n",
      "Requirement already satisfied: grpcio==1.50.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 71)) (1.50.0)\n",
      "Requirement already satisfied: gunicorn==20.1.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 72)) (20.1.0)\n",
      "Requirement already satisfied: h11==0.14.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 73)) (0.14.0)\n",
      "Requirement already satisfied: h5py==3.7.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 74)) (3.7.0)\n",
      "Requirement already satisfied: haystack==0.42 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 75)) (0.42)\n",
      "Requirement already satisfied: html-text==0.5.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 76)) (0.5.2)\n",
      "Requirement already satisfied: html5lib==1.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 77)) (1.1)\n",
      "Requirement already satisfied: htmlmin==0.1.12 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 78)) (0.1.12)\n",
      "Requirement already satisfied: httpcore==0.17.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 79)) (0.17.1)\n",
      "Requirement already satisfied: httpx==0.24.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 80)) (0.24.1)\n",
      "Requirement already satisfied: huggingface-hub==0.14.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 81)) (0.14.1)\n",
      "Requirement already satisfied: idna==3.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 82)) (3.4)\n",
      "Requirement already satisfied: ImageHash==4.3.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 83)) (4.3.1)\n",
      "Requirement already satisfied: importlib-metadata==5.0.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 84)) (5.0.0)\n",
      "Requirement already satisfied: inflect==6.0.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 85)) (6.0.2)\n",
      "Requirement already satisfied: iniconfig==1.1.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 86)) (1.1.1)\n",
      "Requirement already satisfied: ipykernel==6.17.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 87)) (6.17.1)\n",
      "Requirement already satisfied: ipython==8.6.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 88)) (8.6.0)\n",
      "Requirement already satisfied: ipywidgets==8.0.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 89)) (8.0.2)\n",
      "Requirement already satisfied: isodate==0.6.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 90)) (0.6.1)\n",
      "Requirement already satisfied: itsdangerous==2.1.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 91)) (2.1.2)\n",
      "Requirement already satisfied: jaconv==0.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 92)) (0.3)\n",
      "Requirement already satisfied: jamo==0.4.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 93)) (0.4.1)\n",
      "Requirement already satisfied: jarowinkler==1.2.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 94)) (1.2.3)\n",
      "Requirement already satisfied: jedi==0.18.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 95)) (0.18.1)\n",
      "Requirement already satisfied: Jinja2==3.1.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 96)) (3.1.2)\n",
      "Requirement already satisfied: joblib==1.2.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 97)) (1.2.0)\n",
      "Requirement already satisfied: jsonschema==4.17.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 98)) (4.17.3)\n",
      "Requirement already satisfied: jstyleson==0.0.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 99)) (0.0.2)\n",
      "Requirement already satisfied: jupyter_client==7.4.7 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 100)) (7.4.7)\n",
      "Requirement already satisfied: jupyter_core==5.0.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 101)) (5.0.0)\n",
      "Requirement already satisfied: jupyterlab-widgets==3.0.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 102)) (3.0.3)\n",
      "Requirement already satisfied: keras==2.10.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 103)) (2.10.0)\n",
      "Requirement already satisfied: Keras-Preprocessing==1.1.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 104)) (1.1.2)\n",
      "Requirement already satisfied: kiwisolver==1.4.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 105)) (1.4.4)\n",
      "Requirement already satisfied: langchain==0.0.173 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 106)) (0.0.173)\n",
      "Requirement already satisfied: langdetect==1.0.9 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 107)) (1.0.9)\n",
      "Requirement already satisfied: libclang==14.0.6 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 108)) (14.0.6)\n",
      "Requirement already satisfied: linkify-it-py==2.0.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 109)) (2.0.2)\n",
      "Requirement already satisfied: llvmlite==0.39.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 110)) (0.39.1)\n",
      "Requirement already satisfied: lxml==4.9.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 111)) (4.9.1)\n",
      "Requirement already satisfied: Mako==1.2.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 112)) (1.2.4)\n",
      "Requirement already satisfied: Markdown==3.4.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 113)) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py==2.2.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 114)) (2.2.0)\n",
      "Requirement already satisfied: markdown2==2.4.6 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 115)) (2.4.6)\n",
      "Requirement already satisfied: MarkupSafe==2.1.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 116)) (2.1.2)\n",
      "Requirement already satisfied: marshmallow==3.19.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 117)) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum==1.5.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 118)) (1.5.1)\n",
      "Requirement already satisfied: matplotlib==3.7.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 119)) (3.7.1)\n",
      "Requirement already satisfied: matplotlib-inline==0.1.6 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 120)) (0.1.6)\n",
      "Requirement already satisfied: mdit-py-plugins==0.3.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 121)) (0.3.3)\n",
      "Requirement already satisfied: mdurl==0.1.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 122)) (0.1.2)\n",
      "Requirement already satisfied: mf2py==1.1.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 123)) (1.1.2)\n",
      "Requirement already satisfied: mistune==2.0.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 124)) (2.0.4)\n",
      "Requirement already satisfied: mlflow==2.0.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 125)) (2.0.1)\n",
      "Requirement already satisfied: mmh3==3.0.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 126)) (3.0.0)\n",
      "Requirement already satisfied: monotonic==1.6 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 127)) (1.6)\n",
      "Requirement already satisfied: more-itertools==9.0.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 128)) (9.0.0)\n",
      "Requirement already satisfied: mpmath==1.2.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 129)) (1.2.1)\n",
      "Requirement already satisfied: msgpack==1.0.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 130)) (1.0.4)\n",
      "Requirement already satisfied: msrest==0.7.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 131)) (0.7.1)\n",
      "Requirement already satisfied: multidict==6.0.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 132)) (6.0.4)\n",
      "Requirement already satisfied: multimethod==1.9 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 133)) (1.9)\n",
      "Requirement already satisfied: multiprocess==0.70.14 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 134)) (0.70.14)\n",
      "Requirement already satisfied: mypy-extensions==0.4.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 135)) (0.4.3)\n",
      "Requirement already satisfied: nest-asyncio==1.5.6 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 136)) (1.5.6)\n",
      "Requirement already satisfied: networkx==2.8.8 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 137)) (2.8.8)\n",
      "Requirement already satisfied: nltk==3.7 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 138)) (3.7)\n",
      "Requirement already satisfied: num2words==0.5.12 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 139)) (0.5.12)\n",
      "Requirement already satisfied: numba==0.56.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 140)) (0.56.4)\n",
      "Requirement already satisfied: numexpr==2.8.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 141)) (2.8.4)\n",
      "Requirement already satisfied: numpy==1.24.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 142)) (1.24.3)\n",
      "Requirement already satisfied: oauthlib==3.2.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 143)) (3.2.2)\n",
      "Requirement already satisfied: openai==0.27.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 144)) (0.27.2)\n",
      "Requirement already satisfied: openapi-schema-pydantic==1.2.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 145)) (1.2.4)\n",
      "Requirement already satisfied: opencv-python==4.7.0.72 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 146)) (4.7.0.72)\n",
      "Requirement already satisfied: openpyxl==3.0.10 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 147)) (3.0.10)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 148)) (3.3.0)\n",
      "Requirement already satisfied: orjson==3.8.12 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 149)) (3.8.12)\n",
      "Requirement already satisfied: outcome==1.2.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 150)) (1.2.0)\n",
      "Requirement already satisfied: packaging==23.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 151)) (23.1)\n",
      "Requirement already satisfied: pandas==2.0.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 152)) (2.0.1)\n",
      "Requirement already satisfied: pandas-profiling==3.6.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 153)) (3.6.0)\n",
      "Requirement already satisfied: pandocfilters==1.5.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 154)) (1.5.0)\n",
      "Requirement already satisfied: parso==0.8.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 155)) (0.8.3)\n",
      "Requirement already satisfied: pathspec==0.10.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 156)) (0.10.2)\n",
      "Requirement already satisfied: patsy==0.5.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 157)) (0.5.3)\n",
      "Requirement already satisfied: pdf2image==1.16.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 158)) (1.16.0)\n",
      "Requirement already satisfied: pdfminer.six==20221105 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 159)) (20221105)\n",
      "Requirement already satisfied: pefile==2022.5.30 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 160)) (2022.5.30)\n",
      "Requirement already satisfied: pexpect==4.8.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 161)) (4.8.0)\n",
      "Requirement already satisfied: phik==0.12.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 162)) (0.12.3)\n",
      "Requirement already satisfied: pickleshare==0.7.5 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 163)) (0.7.5)\n",
      "Requirement already satisfied: Pillow==9.5.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 164)) (9.5.0)\n",
      "Requirement already satisfied: platformdirs==2.5.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 165)) (2.5.4)\n",
      "Requirement already satisfied: pluggy==1.0.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 166)) (1.0.0)\n",
      "Requirement already satisfied: posthog==2.2.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 167)) (2.2.0)\n",
      "Requirement already satisfied: prompt-toolkit==3.0.32 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 168)) (3.0.32)\n",
      "Requirement already satisfied: protobuf==3.19.6 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 169)) (3.19.6)\n",
      "Requirement already satisfied: psutil==5.9.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 170)) (5.9.4)\n",
      "Requirement already satisfied: psycopg2-binary==2.9.5 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 171)) (2.9.5)\n",
      "Requirement already satisfied: ptyprocess==0.7.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 172)) (0.7.0)\n",
      "Requirement already satisfied: pure-eval==0.2.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 173)) (0.2.2)\n",
      "Requirement already satisfied: py==1.11.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 174)) (1.11.0)\n",
      "Requirement already satisfied: py-cpuinfo==9.0.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 175)) (9.0.0)\n",
      "Requirement already satisfied: pyarrow==10.0.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 176)) (10.0.0)\n",
      "Requirement already satisfied: pyasn1==0.4.8 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 177)) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules==0.2.8 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 178)) (0.2.8)\n",
      "Requirement already satisfied: pycparser==2.21 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 179)) (2.21)\n",
      "Requirement already satisfied: pycryptodomex==3.16.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 180)) (3.16.0)\n",
      "Requirement already satisfied: pydantic==1.10.7 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 181)) (1.10.7)\n",
      "Requirement already satisfied: pydub==0.25.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 182)) (0.25.1)\n",
      "Requirement already satisfied: Pygments==2.13.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 183)) (2.13.0)\n",
      "Requirement already satisfied: PyJWT==2.6.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 184)) (2.6.0)\n",
      "Requirement already satisfied: pyparsing==3.0.9 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 185)) (3.0.9)\n",
      "Requirement already satisfied: PyPDF2==2.12.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 186)) (2.12.1)\n",
      "Requirement already satisfied: pypinyin==0.44.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 187)) (0.44.0)\n",
      "Requirement already satisfied: pyRdfa3==3.5.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 188)) (3.5.3)\n",
      "Requirement already satisfied: pyrsistent==0.19.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 189)) (0.19.3)\n",
      "Requirement already satisfied: PySocks==1.7.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 190)) (1.7.1)\n",
      "Requirement already satisfied: pytesseract==0.3.10 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 191)) (0.3.10)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 192)) (2.8.2)\n",
      "Requirement already satisfied: python-docx==0.8.11 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 193)) (0.8.11)\n",
      "Requirement already satisfied: python-dotenv==0.21.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 194)) (0.21.0)\n",
      "Requirement already satisfied: python-magic==0.4.27 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 195)) (0.4.27)\n",
      "Requirement already satisfied: python-multipart==0.0.6 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 196)) (0.0.6)\n",
      "Requirement already satisfied: python-ptrace==0.9.8 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 197)) (0.9.8)\n",
      "Requirement already satisfied: pytorch-wpe==0.0.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 198)) (0.0.1)\n",
      "Requirement already satisfied: pytrec-eval==0.5 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 199)) (0.5)\n",
      "Requirement already satisfied: pytz==2023.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 200)) (2023.3)\n",
      "Requirement already satisfied: PyWavelets==1.4.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 201)) (1.4.1)\n",
      "Requirement already satisfied: PyYAML==6.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 202)) (6.0)\n",
      "Requirement already satisfied: pyzmq==24.0.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 203)) (24.0.1)\n",
      "Requirement already satisfied: quantulum3==0.7.11 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 204)) (0.7.11)\n",
      "Requirement already satisfied: querystring-parser==1.2.4 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 205)) (1.2.4)\n",
      "Requirement already satisfied: rank-bm25==0.2.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 206)) (0.2.2)\n",
      "Requirement already satisfied: rapidfuzz==2.7.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 207)) (2.7.0)\n",
      "Requirement already satisfied: rdflib==6.2.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 208)) (6.2.0)\n",
      "Requirement already satisfied: regex==2022.10.31 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 209)) (2022.10.31)\n",
      "Requirement already satisfied: requests==2.30.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 210)) (2.30.0)\n",
      "Requirement already satisfied: requests-oauthlib==1.3.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 211)) (1.3.1)\n",
      "Requirement already satisfied: responses==0.18.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 212)) (0.18.0)\n",
      "Requirement already satisfied: rsa==4.9 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 213)) (4.9)\n",
      "Requirement already satisfied: scikit-learn==1.1.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 214)) (1.1.3)\n",
      "Requirement already satisfied: scipy==1.9.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 215)) (1.9.3)\n",
      "Requirement already satisfied: seaborn==0.12.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 216)) (0.12.1)\n",
      "Requirement already satisfied: semantic-version==2.10.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 217)) (2.10.0)\n",
      "Requirement already satisfied: sentence-transformers==2.2.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 218)) (2.2.2)\n",
      "Requirement already satisfied: sentencepiece==0.1.97 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 219)) (0.1.97)\n",
      "Requirement already satisfied: seqeval==1.2.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 220)) (1.2.2)\n",
      "Requirement already satisfied: shap==0.41.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 221)) (0.41.0)\n",
      "Requirement already satisfied: six==1.16.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 222)) (1.16.0)\n",
      "Requirement already satisfied: slicer==0.0.7 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 223)) (0.0.7)\n",
      "Requirement already satisfied: smmap==5.0.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 224)) (5.0.0)\n",
      "Requirement already satisfied: sniffio==1.3.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 225)) (1.3.0)\n",
      "Requirement already satisfied: sortedcontainers==2.4.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 226)) (2.4.0)\n",
      "Requirement already satisfied: soupsieve==2.3.2.post1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 227)) (2.3.2.post1)\n",
      "Requirement already satisfied: SQLAlchemy==1.4.44 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 228)) (1.4.44)\n",
      "Requirement already satisfied: SQLAlchemy-Utils==0.38.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 229)) (0.38.3)\n",
      "Requirement already satisfied: sqlparse==0.4.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 230)) (0.4.3)\n",
      "Requirement already satisfied: stack-data==0.6.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 231)) (0.6.1)\n",
      "Requirement already satisfied: starlette==0.27.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 232)) (0.27.0)\n",
      "Requirement already satisfied: statsmodels==0.13.5 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 233)) (0.13.5)\n",
      "Requirement already satisfied: sympy==1.11.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 234)) (1.11.1)\n",
      "Requirement already satisfied: tabula-py==2.7.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 235)) (2.7.0)\n",
      "Requirement already satisfied: tabulate==0.9.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 236)) (0.9.0)\n",
      "Requirement already satisfied: tangled-up-in-unicode==0.2.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 237)) (0.2.0)\n",
      "Requirement already satisfied: tenacity==8.2.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 238)) (8.2.2)\n",
      "Requirement already satisfied: tensorboard==2.10.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 239)) (2.10.1)\n",
      "Requirement already satisfied: tensorboard-data-server==0.6.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 240)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit==1.8.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 241)) (1.8.1)\n",
      "Requirement already satisfied: tensorflow-estimator==2.10.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 242)) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.10.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 243)) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-metal==0.6.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 244)) (0.6.0)\n",
      "Requirement already satisfied: termcolor==2.1.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 245)) (2.1.0)\n",
      "Requirement already satisfied: threadpoolctl==3.1.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 246)) (3.1.0)\n",
      "Requirement already satisfied: tika==1.24 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 247)) (1.24)\n",
      "Requirement already satisfied: tiktoken==0.4.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 248)) (0.4.0)\n",
      "Requirement already satisfied: tinycss2==1.2.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 249)) (1.2.1)\n",
      "Requirement already satisfied: tk==0.1.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 250)) (0.1.0)\n",
      "Requirement already satisfied: tokenize-rt==5.0.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 251)) (5.0.0)\n",
      "Requirement already satisfied: tokenizers==0.12.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 252)) (0.12.1)\n",
      "Requirement already satisfied: toml==0.10.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 253)) (0.10.2)\n",
      "Requirement already satisfied: tomli==2.0.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 254)) (2.0.1)\n",
      "Requirement already satisfied: tomli_w==1.0.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 255)) (1.0.0)\n",
      "Requirement already satisfied: tomlkit==0.11.6 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 256)) (0.11.6)\n",
      "Requirement already satisfied: toolz==0.12.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 257)) (0.12.0)\n",
      "Requirement already satisfied: torch==1.12.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 258)) (1.12.1)\n",
      "Requirement already satisfied: torch-complex==0.4.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 259)) (0.4.3)\n",
      "Requirement already satisfied: torchvision==0.14.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 260)) (0.14.0)\n",
      "Requirement already satisfied: tornado==6.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 261)) (6.2)\n",
      "Requirement already satisfied: tqdm==4.65.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 262)) (4.65.0)\n",
      "Requirement already satisfied: traitlets==5.5.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 263)) (5.5.0)\n",
      "Collecting transformers==4.25.1\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: typeguard==2.13.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 265)) (2.13.3)\n",
      "Requirement already satisfied: typing-inspect==0.8.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 266)) (0.8.0)\n",
      "Requirement already satisfied: typing_extensions==4.5.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 267)) (4.5.0)\n",
      "Requirement already satisfied: tzdata==2023.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 268)) (2023.3)\n",
      "Requirement already satisfied: uc-micro-py==1.0.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 269)) (1.0.2)\n",
      "Requirement already satisfied: ujson==5.1.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 270)) (5.1.0)\n",
      "Requirement already satisfied: Unidecode==1.3.6 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 271)) (1.3.6)\n",
      "Requirement already satisfied: url-normalize==1.4.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 272)) (1.4.3)\n",
      "Requirement already satisfied: urllib3==2.0.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 273)) (2.0.2)\n",
      "Requirement already satisfied: uvicorn==0.22.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 274)) (0.22.0)\n",
      "Requirement already satisfied: validators==0.18.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 275)) (0.18.2)\n",
      "Requirement already satisfied: virtualenv==20.16.7 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 276)) (20.16.7)\n",
      "Requirement already satisfied: visions==0.7.5 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 277)) (0.7.5)\n",
      "Requirement already satisfied: w3lib==2.1.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 278)) (2.1.1)\n",
      "Requirement already satisfied: watchdog==2.1.9 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 279)) (2.1.9)\n",
      "Requirement already satisfied: wcwidth==0.2.5 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 280)) (0.2.5)\n",
      "Requirement already satisfied: webencodings==0.5.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 281)) (0.5.1)\n",
      "Requirement already satisfied: websocket-client==1.4.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 282)) (1.4.2)\n",
      "Requirement already satisfied: websockets==11.0.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 283)) (11.0.3)\n",
      "Requirement already satisfied: Werkzeug==2.2.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 284)) (2.2.2)\n",
      "Requirement already satisfied: widgetsnbextension==4.0.3 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 285)) (4.0.3)\n",
      "Requirement already satisfied: wrapt==1.14.1 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 286)) (1.14.1)\n",
      "Requirement already satisfied: xlwt==1.3.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 287)) (1.3.0)\n",
      "Requirement already satisfied: xmltodict==0.13.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 288)) (0.13.0)\n",
      "Requirement already satisfied: xxhash==3.1.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 289)) (3.1.0)\n",
      "Requirement already satisfied: yapf==0.32.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 290)) (0.32.0)\n",
      "Requirement already satisfied: yarl==1.9.2 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 291)) (1.9.2)\n",
      "Requirement already satisfied: zipp==3.10.0 in ./enviromentSD/lib/python3.10/site-packages (from -r requirements.txt (line 292)) (3.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./enviromentSD/lib/python3.10/site-packages (from astunparse==1.6.3->-r requirements.txt (line 11)) (0.38.4)\n",
      "INFO: pip is looking at multiple versions of beautifulsoup4 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting beautifulsoup4==4.11.1\n",
      "  Using cached beautifulsoup4-4.11.1-py3-none-any.whl (128 kB)\n",
      "INFO: pip is looking at multiple versions of backoff to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting backoff==2.2.1\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "INFO: pip is looking at multiple versions of backcall to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting backcall==0.2.0\n",
      "  Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
      "INFO: pip is looking at multiple versions of azure-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting azure-core==1.26.1\n",
      "  Using cached azure_core-1.26.1-py3-none-any.whl (172 kB)\n",
      "INFO: pip is looking at multiple versions of azure-common to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting azure-common==1.1.28\n",
      "  Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "INFO: pip is looking at multiple versions of azure-ai-formrecognizer to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting azure-ai-formrecognizer==3.2.0\n",
      "  Using cached azure_ai_formrecognizer-3.2.0-py3-none-any.whl (228 kB)\n",
      "INFO: pip is looking at multiple versions of attrs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting attrs==23.1.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "INFO: pip is looking at multiple versions of async-timeout to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting async-timeout==4.0.2\n",
      "  Using cached async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "INFO: pip is looking at multiple versions of astunparse to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "INFO: pip is looking at multiple versions of asttokens to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting asttokens==2.1.0\n",
      "  Using cached asttokens-2.1.0-py2.py3-none-any.whl (26 kB)\n",
      "INFO: pip is looking at multiple versions of appnope to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting appnope==0.1.3\n",
      "  Using cached appnope-0.1.3-py2.py3-none-any.whl (4.4 kB)\n",
      "INFO: pip is looking at multiple versions of appdirs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting appdirs==1.4.4\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "INFO: pip is looking at multiple versions of anyio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting anyio==3.6.2\n",
      "  Using cached anyio-3.6.2-py3-none-any.whl (80 kB)\n",
      "INFO: pip is looking at multiple versions of altair to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting altair==5.0.0\n",
      "  Using cached altair-5.0.0-py3-none-any.whl (477 kB)\n",
      "INFO: pip is looking at multiple versions of alembic to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting alembic==1.8.1\n",
      "  Using cached alembic-1.8.1-py3-none-any.whl (209 kB)\n",
      "INFO: pip is looking at multiple versions of aiosignal to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiosignal==1.3.1\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "INFO: pip is looking at multiple versions of aiohttp to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiohttp==3.8.4\n",
      "  Using cached aiohttp-3.8.4-cp310-cp310-macosx_11_0_arm64.whl (336 kB)\n",
      "INFO: pip is looking at multiple versions of aiofiles to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting aiofiles==23.1.0\n",
      "  Using cached aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
      "INFO: pip is looking at multiple versions of absl-py to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting absl-py==1.3.0\n",
      "  Using cached absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
      "\u001b[31mERROR: Cannot install blobfile==2.0.0 and urllib3==2.0.2 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    The user requested urllib3==2.0.2\n",
      "    blobfile 2.0.0 depends on urllib3~=1.25\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Generating metadata for package faiss-gpu produced metadata for project name faiss-cpu. Fix your #egg=faiss-gpu fragments.\u001b[0m\u001b[33m\n",
      "\u001b[0m  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[7 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m creating /private/var/folders/xf/wd72s5c13p941b_5lpj9wfrw0000gn/T/pip-pip-egg-info-rgso624d/faiss_cpu.egg-info\n",
      "  \u001b[31m   \u001b[0m writing /private/var/folders/xf/wd72s5c13p941b_5lpj9wfrw0000gn/T/pip-pip-egg-info-rgso624d/faiss_cpu.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to /private/var/folders/xf/wd72s5c13p941b_5lpj9wfrw0000gn/T/pip-pip-egg-info-rgso624d/faiss_cpu.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to /private/var/folders/xf/wd72s5c13p941b_5lpj9wfrw0000gn/T/pip-pip-egg-info-rgso624d/faiss_cpu.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m writing manifest file '/private/var/folders/xf/wd72s5c13p941b_5lpj9wfrw0000gn/T/pip-pip-egg-info-rgso624d/faiss_cpu.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m error: package directory '/private/var/folders/xf/wd72s5c13p941b_5lpj9wfrw0000gn/T/pip-install-hlwc3ufi/faiss-gpu_a60a30570ee048bf80609f7504a01654/faiss/faiss/python' does not exist\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Dependencies\n",
    "# In order to extract the tables, you need java installed in your system\n",
    "!java -version\n",
    "\n",
    "# Install the requirements.txt of the repo\n",
    "!pip install -r requirements.txt\n",
    "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
    "!pip install -q datasets loralib sentencepiece \n",
    "!pip -q install bitsandbytes accelerate\n",
    "!pip -q install langchain\n",
    "!pip -q install tiktoken\n",
    "!pip -q install openai\n",
    "!pip -q install faiss-gpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15764563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, GenerationConfig, pipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ec43e3",
   "metadata": {},
   "source": [
    "## Configurable parameters\n",
    "The following cell contains the parameters you need to configure in order to use this notebook.\n",
    "In resume: \n",
    "\n",
    "1- You need to point the DocumentPath to a .txt file of the dataset you canto analyze\n",
    "2 - You need to set a name for the Output file via OutputFileName\n",
    "3 - You need to set your API KEY form OpenAI or HF\n",
    "\n",
    "Also:\n",
    "\n",
    "4 - You can selecte the model to use. (Be aware that FLAN-UL2 needs at lest an NVIDIA A100 GPU)\n",
    "\n",
    "Once finished all the cells, you will find a file with the results in the root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76d6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "## Variables to configure\n",
    "## Select the cleaned dataset file you want to process\n",
    "documentPath = \"sources/Nature-Scientific-Data/A whole-body FDG.txt\"\n",
    "\n",
    "## Set the name of the generated output file with the results (in .xlsx)\n",
    "outputfileName = \"Whole-body Results\"\n",
    "\n",
    "## Table Extraction\n",
    "## Set to 0 if no table extraction is neded\n",
    "## Tables are already extracted in the source files, so keep it to 0.\n",
    "extract_tables = 0\n",
    "## The PDF where the tables are.\n",
    "# pdf_path =\"sources/Nature-Scientific-Data/A whole-body FDG-PET:CT.pdf\"\n",
    "\n",
    "## To adapt the approach to your domain specific use-cases, configure your own semantic dictionary\n",
    "context = {\n",
    "    \"title\": documentPath.split(\"/\")[-1],  \n",
    "    \"gathering\": [\"collection\",\"gathering\", \"acquisition\"],\n",
    "    \"annotation\":[\"labeling\", \"annotation\"],\n",
    "    \"statistics\": [\"Characteristics\", \"Statistics\", \"Features\"],\n",
    "}\n",
    "\n",
    "## NOTE: You will need a APIKEY for OPENAI to use text-davinci-003, or a Hugginface API TOKEN (free) to download the FLAN-UL2\n",
    "api_key=os.getenv(\"OPEN_AI_API_KEY\")\n",
    "api_key_t5=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "## Select the model to use during the experiment\n",
    "## NOTE: Using texct-davinci-003 will not require special hardware requirements. You can execute it in your local setup\n",
    "model = \"text-davinci-003\" \n",
    "## NOTE: Using FLAN-UL2 will require to have at least 40GB of VRAM on your system. \n",
    "## So, FLAN-UL2 will not work on a common local setup. The experiments have beend one using a Nvidia A100 GPU. \n",
    "#model = \"UL2\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e7ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the libraries\n",
    "from langchain.llms import OpenAI, Cohere\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "if model == \"UL2\":\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration\n",
    "    model_id = 'google/flan-ul2'# go for a smaller model if you dont have the VRAM\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-ul2\")\n",
    "    pipe = pipeline(\n",
    "        model,\n",
    "        tokenizer=tokenizer, \n",
    "        max_length=128,\n",
    "        temperature=0.0,\n",
    "        device_map=\"auto\", \n",
    "        model_kwargs={\"load_in_8bit\": True}\n",
    "    )\n",
    "\n",
    "    LLMClient = HuggingFacePipeline(pipeline=pipe)\n",
    "    retrieved_docs = 10\n",
    "\n",
    "elif model == \"text-davinci-003\":\n",
    "    LLMClient = OpenAI(model_name=model, openai_api_key=api_key,temperature=0)\n",
    "    retrieved_docs = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f0efc",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "First we prepare the data. For this example we do similarity search over a vector database, but these documents could be fetched in any manner (the point of this notebook to highlight what to do AFTER you fetch the documents).\n",
    "### Text and Question preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f28130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8979985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 284, which is longer than the specified 200\n",
      "Created a chunk of size 564, which is longer than the specified 200\n",
      "Created a chunk of size 294, which is longer than the specified 200\n",
      "Created a chunk of size 477, which is longer than the specified 200\n",
      "Created a chunk of size 247, which is longer than the specified 200\n",
      "Created a chunk of size 294, which is longer than the specified 200\n",
      "Created a chunk of size 867, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering\n",
      "gathering: 0\n",
      "collection\n",
      "collection: 2\n",
      "acquisition\n",
      "acquisition: 5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Import the documentation\n",
    "with open(documentPath) as f:\n",
    "    documentation = f.read()\n",
    "\n",
    "# Configure the text splitter and embeddings\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=200, chunk_overlap=10)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "\n",
    "# Configure semantic dictionary\n",
    "gathering = {\"gathering\": 0}\n",
    "semdict = {\"gathering\": [\"gathering\", \"collection\", \"acquisition\"]}\n",
    "for concept, words in semdict.items():\n",
    "    for word in words:\n",
    "         print(word)\n",
    "         count = sum(1 for match in re.finditer(word, documentation))\n",
    "         print(word+\": \"+ str(count))\n",
    "         if count > gathering[\"gathering\"]:\n",
    "            gathering[\"gathering\"] = count\n",
    "            context['gathering'] = word\n",
    "\n",
    "\n",
    "# Split, encode and index the text\n",
    "texts = text_splitter.split_text(documentation)\n",
    "for idx, text in enumerate(texts):\n",
    "    texts[idx] = text.replace('\\n',' ')\n",
    "    \n",
    "docsearch = FAISS.from_texts(texts, embeddings, metadatas=[{\"source\": i} for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68afef",
   "metadata": {},
   "source": [
    "### Prompt types preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3722373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "## Incontext prompt and refine prompt\n",
    "chain_refine = load_qa_chain(LLMClient, chain_type=\"refine\",return_refine_steps=True)\n",
    "incontext_prompt = load_qa_chain(LLMClient, chain_type=\"stuff\")\n",
    "\n",
    "## Prompt type to reduce the context size, for LLM with lower contexts window.\n",
    "chain_reduce = load_qa_chain(LLMClient, chain_type=\"map_reduce\", return_intermediate_steps=True)\n",
    "\n",
    "## Custom instruction prompt type (Classification and Parsing)\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "{instruction}\n",
    "###\n",
    "Context: \n",
    "{context}\n",
    "###\n",
    "Question: {question}\n",
    "###\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "instruction_prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\",\"context\",\"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "chain_instruction_simple = LLMChain(llm=LLMClient, prompt=instruction_prompt)\n",
    "\n",
    "## Table prompt to transform parsed tables in natural text\n",
    "prompt_template = \"\"\"Given the following table in HTML, and the given context related the table: Translate the content of the table into natural language.\n",
    "###\n",
    "Context: \n",
    "{context}\n",
    "###\n",
    "Table: {table}\n",
    "###\n",
    "Table translation:\n",
    "\"\"\"\n",
    "table_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\",\"table\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "chain_table = LLMChain(llm=LLMClient, prompt=table_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7644d",
   "metadata": {},
   "source": [
    "### Table Extraction and preparation\n",
    "\n",
    "Tables already extracted for this dataset, so not necessary to execute the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b67c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tables\n",
    "if (extract_tables == 1):\n",
    "    import tabula ## You need to have the Java Tabula installed in the environment\n",
    "    table_texts = []\n",
    "    dfs = tabula.read_pdf(pdf_path, pages='all')\n",
    "    for idx, table in enumerate(dfs):\n",
    "        query = \"Table \"+str(idx+1)+\":\"\n",
    "        docs = docsearch.similarity_search(query, k=4)\n",
    "        result = chain_table({\"context\":docs,\"table\":table})\n",
    "        print(query + \" \"+ result['text'])\n",
    "        table_texts.append(query + \" \"+ result['text'])\n",
    "        # Building the in-context chain with specific instructions\n",
    "    docsearch.add_texts(table_texts,metadatas=[{\"source\": i} for i in range(len(texts))])\n",
    "    with open(documentPath, 'a') as f:\n",
    "        for line in table_texts:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d825b7",
   "metadata": {},
   "source": [
    "# Extraction process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26e60e",
   "metadata": {},
   "source": [
    "## Uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23dc7ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b927fa94",
   "metadata": {},
   "source": [
    "### Description and Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41177484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The purpose of the dataset is to provide a publicly available dataset of annotated Positron Emission Tomography/Computed Tomography (PET/CT) studies for deep learning-based automated analysis of PET/CT data.\n"
     ]
    }
   ],
   "source": [
    "# Purposes\n",
    "specificPart = {\n",
    "    \"id\":'metadata.description.purposes', \n",
    "    \"questions\":[\"\"\"Which are the purpose or purposes of the dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"refine\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0], k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0]}, return_only_outputs=True)\n",
    "specificPart['result'] = result['output_text']\n",
    "results.append(specificPart)\n",
    "print(result['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ea8b52d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks: Get the the tasks of the dataset with the docs of the purposes. \n",
    "## We get context from previous answer\n",
    "specificPart = {\n",
    "    \"id\":'metadata.description.tasks', \n",
    "    \"questions\":[\"\"\"Which of the following tasks is the dataset inteded for?:\n",
    "\n",
    "            text-classification, question-answering, text-generation, token-classification, translation,\n",
    "            fill-mask, text-retrieval, conditional-text-generation, sequence-modeling, summarization, other,\n",
    "            structure-prediction, information-retrieval, text2text-generation, zero-shot-retrieval,\n",
    "            zero-shot-information-retrieval, automatic-speech-recognition, image-classification, speech-processing,\n",
    "            text-scoring, audio-classification, conversational, question-generation, image-to-text, data-to-text,\n",
    "            classification, object-detection, multiple-choice, text-mining, image-segmentation, dialog-response-generation,\n",
    "            named-entity-recognition, sentiment-analysis, machine-translation, tabular-to-text, table-to-text, simplification,\n",
    "            sentence-similarity, zero-shot-classification, visual-question-answering, text_classification, time-series-forecasting,\n",
    "            computer-vision, feature-extraction, symbolic-regression, topic modeling, one liner summary, email subject, meeting title,\n",
    "            text-to-structured, reasoning, paraphrasing, paraphrase, code-generation, tts, image-retrieval, image-captioning,\n",
    "            language-modelling, video-captionning, neural-machine-translation, transkation, text-generation-other-common-sense-inference,\n",
    "            text-generation-other-discourse-analysis, text-to-tabular, text-generation-other-code-modeling, other-text-search\n",
    "\n",
    "            If you are not sure answer with just with \"others\".\n",
    "            Please, answer only with the one or some of the provided tasks separated by commas. \"\"\"],\n",
    "    \"promptStrategy\": \"classification\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0], k=9)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart['questions'][0]},return_only_outputs=True)\n",
    "specificPart['result'] = result['output_text']\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "58a38d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the tags of the dataset from the doc of purposes\n",
    "## We get context from previous answer\n",
    "specificPart = {\n",
    "    \"id\":'metadata.description.tags', \n",
    "    \"questions\":[\"\"\"Given the context information can you generate a set of representative keywords of it? Please provide the tags comma separated.\"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "}\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart['questions'][0]},return_only_outputs=True)\n",
    "specificPart['result'] = result['output_text']\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f98c8fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "# Gaps\n",
    "specificPart = {\n",
    "    \"id\":'metadata.description.gaps', \n",
    "    \"questions\":[\"\"\"Which are the gaps the  \"\"\"+ context['title']+ \"\"\" dataset intend to fill?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"reduce\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0], k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0],\"token_max\":1800}, return_only_outputs=True)\n",
    "specificPart['result'] = result['output_text']\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee9055a",
   "metadata": {},
   "source": [
    "### Recommnedations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "928bd105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "## Reccomendations\n",
    "# Recommended\n",
    "question = \"\"\"For which applications the \"\"\"+ context['title']+ \"\"\"   dataset is recommended?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "    \"id\":'metadata.applications.recommended', \n",
    "    \"questions\":[question],\n",
    "    \"promptStrategy\": \"refine\",\n",
    "    \"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "# Non-Recommended\n",
    "question = \"\"\"Is there any non-recommneded application for the \"\"\"+ context['title']+ \"\"\"   dataset? If you are not sure, or there is any non-recommended use of the dataset metioned in the context, just answer with \"no\".\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "    \"id\":'metadata.applications.non_recommended', \n",
    "    \"questions\":[question],\n",
    "    \"promptStrategy\": \"refine\",\n",
    "    \"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa4dff",
   "metadata": {},
   "source": [
    "### ML benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "223a9530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " YES\n",
      " nnUNet13\n",
      "['nnUNet13']\n",
      " nnUNet13:  Dice score, false positive volume, and false negative volume.\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "## Benchmarking\n",
    "question1 = \"\"\"Has the  \"\"\"+ context['title']+ \"\"\"  dataset been tested using any Machine learning technique?\n",
    "                Answer only with a YES or NO\n",
    "                If you are not sure answer with UNSURE\n",
    "                \"\"\"\n",
    "docs = docsearch.similarity_search(question1, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question1},return_only_outputs=True)\n",
    "print(result['output_text'])\n",
    "\n",
    "if \"yes\" in result['output_text'] or \"Yes\" in result['output_text'] or \"YES\" in result['output_text'] :\n",
    "    questionModelNames = \"\"\"Which are the name of the models used to test the dataset? If there is more than one, please provide a list of the models name comma separated\n",
    "                   example answer: Model 1, Model 2, Model 3 \n",
    "                    \n",
    "                    \"\"\"\n",
    "    #docs = docsearch.similarity_search(question2, k=retrieved_docs)\n",
    "    result_sub = incontext_prompt({\"input_documents\": docs, \"question\": questionModelNames},return_only_outputs=True)\n",
    "    models_parsed = parser.parse(result_sub['output_text'])\n",
    "    print (result_sub['output_text'])\n",
    "    print(models_parsed)\n",
    "    result_complete = \"\"\n",
    "    for model in models_parsed: \n",
    "            # Metrics\n",
    "        questionMetrics = \"\"\"Which are the metrics mentioned in the context of the \"\"\"+model+\"\"\" approach?\n",
    "                       If there are no metric just answer with \"no metrics\"\n",
    "                        \"\"\"\n",
    "        # docs = docsearch.similarity_search(questionMetrics, k=retrieved_docs)\n",
    "        result_sub = incontext_prompt({\"input_documents\": docs, \"question\": questionMetrics},return_only_outputs=True)\n",
    "        result_complete = result_complete + \" \" + model +\": \" + result_sub[\"output_text\"]\n",
    "        print(result_complete)\n",
    "\n",
    "    specificPart = {\n",
    "        \"id\":'metadata.applications.benchmarking.modelName', \n",
    "        \"questions\":[question1,questionModelNames],\n",
    "        \"promptStrategy\": \"simple\",\n",
    "        \"result\": result_complete\n",
    "        }\n",
    "    results.append(specificPart)\n",
    "else:\n",
    "    specificPart = {\n",
    "    \"id\":'metadata.applications.benchmarking.modelName', \n",
    "    \"questions\":[question1],\n",
    "    \"promptStrategy\": \"simple\",\n",
    "    \"result\": 'not provided'\n",
    "    }\n",
    "    results.append(specificPart)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8040a",
   "metadata": {},
   "source": [
    "## Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "685e0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Authors\n",
    "# Gaps\n",
    "specificPart = {\n",
    "    \"id\":'metadata.authoring.authors', \n",
    "    \"questions\":[\"\"\"Who are the authors of the \"\"\"+ context['title']+ \"\"\" dataset? Please, answer only with the authors name and affiliation separated by commas.\"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0], k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0]},return_only_outputs=True)\n",
    "specificPart['result'] = result['output_text']\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6a7b0146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Funders\n",
    "question = \"\"\"Is there any organization which supported or funded the creation of the dataset?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "## Extract the funder's name\n",
    "results.append(\n",
    "    {\n",
    "    \"id\":'metadata.authoring.fundersName', \n",
    "    \"questions\":[\n",
    "        \"\"\"Is there any organization which supported or funded the creation of the dataset?\"\"\"],\n",
    "    \"promptStrategy\": \"in-context\",\n",
    "    \"result\": result['output_text']\n",
    "})\n",
    "## Try to guess funder's type\n",
    "question = \"\"\"The organization mentioned in this context:\n",
    "\n",
    "CONTEXT: \"\"\"+ result['output_text'] + \"\"\"\n",
    "\n",
    "Are of type public, private organizations?\n",
    "If you are not sure answer with just with \"unknown\" \"\"\"\n",
    "result_sub = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "results.append(\n",
    "    {\n",
    "    \"id\":'metadata.authoring.fundersType', \n",
    "    \"questions\":[\n",
    "        \"\"\"The organization mentioned in this context:\n",
    "\n",
    "        CONTEXT: \"\"\"+ result['output_text'] + \"\"\"\n",
    "\n",
    "        Are of public or private organizations?\n",
    "        If you are not sure answer with just with \"unknown\" \"\"\"],\n",
    "    \"promptStrategy\": \"chained\",\n",
    "    \"result\": result_sub['output_text']\n",
    "})\n",
    "\n",
    "\n",
    "## Extracting grantor ID\n",
    "question = \"\"\"Given the context information:\n",
    "\n",
    "    CONTEXT: \"\"\"+ result['output_text'] + \"\"\"\n",
    "\n",
    "    Is there any ID or reference of the grants provided by the funders?\n",
    "    \n",
    "    If you are not sure, answer \"not provided\"\"\"\n",
    "result_sub = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "results.append(\n",
    "    {\n",
    "    \"id\":'metadata.authoring.grantsID', \n",
    "    \"questions\":[\n",
    "        \"\"\"Given the context information:\n",
    "\n",
    "        CONTEXT: \"\"\"+ result['output_text'] + \"\"\"\n",
    "\n",
    "        Which is the ID of the grant?\n",
    "        \n",
    "        If you are not sure, answer \"not provided\"\"\"],\n",
    "    \"promptStrategy\": \"chained\",\n",
    "    \"result\": result_sub['output_text']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "aed68875",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maintainers\n",
    "specificPart = {\n",
    "    \"id\":'metadata.authoring.maintainers', \n",
    "    \"questions\":[\"\"\"Who are the maintainers of the  \"\"\"+ context['title']+ \"\"\" dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"reduce\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0], k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0], \"token_max\":1800}, return_only_outputs=True)\n",
    "specificPart[\"result\"] = result['output_text']\n",
    "results.append(specificPart)\n",
    "\n",
    "## Contribution Guidelines\n",
    "specificPart= {\n",
    "    \"id\":'metadata.authoring.contribution_guidelines', \n",
    "    \"questions\":[\"\"\"Which are the contribution guidelines of the  \"\"\"+ context['title']+ \"\"\" dataset? If you are not sure, or there is no contribution guidelines just answer with \"no\".\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0],k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0]},return_only_outputs=True)\n",
    "specificPart[\"result\"] = result['output_text']\n",
    "results.append(specificPart)\n",
    "\n",
    "# Erratum\n",
    "specificPart = {\n",
    "    \"id\":'metadata.authoring.erratum', \n",
    "    \"questions\":[\"\"\"Is there any data retention limit in the  \"\"\"+ context['title']+ \"\"\" dataset? If you are not sure, or there is no retention limit just answer with \"no\".\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0], k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0]},return_only_outputs=True)\n",
    "specificPart[\"result\"] = result['output_text']\n",
    "results.append(specificPart)\n",
    "\n",
    "# Retention\n",
    "specificPart = {\n",
    "    \"id\":'metadata.authoring.data_retention', \n",
    "    \"questions\":[\"\"\"Is there any data retention policies policiy of the  \"\"\"+ context['title']+ \"\"\" dataset? If you are not sure, or there is no retention policy just answer with \"no\".\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0])\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0]},return_only_outputs=True)\n",
    "specificPart[\"result\"] = result['output_text']\n",
    "results.append(specificPart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8084fa",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "70151192",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = []\n",
    "## Distribution section\n",
    "concepts.append({\n",
    "    \"id\":'metadata.distribution.data_repository', \n",
    "    \"questions\":[\"\"\"Is there a link to the a repository containing the data? If you are not sure, or there is no link to the repository just answer with \"no\".\"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'metadata.distribution.license', \n",
    "    \"questions\":[\"\"\"Which is the license of the  \"\"\" +context['title']+\"\"\"  dataset. If you are not sure, or there is mention to a license of the dataset in the context, just answer with \"no\".\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'metadata.distribution.rights_of_data', \n",
    "    \"questions\":[\"\"\"Which are the rights of the stand-alone dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'metadata.distribution.rights_of_model', \n",
    "    \"questions\":[\"\"\"Which are the rights of the models trained with this data?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'metadata.distribution.attribution_credits', \n",
    "    \"questions\":[\"\"\"Is there any attribution notice that have to be used to use the {  \"\"\"+ context['title']+ \"\"\" dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append( {\n",
    "    \"id\":'metadata.distribution.designated_third_parties', \n",
    "    \"questions\":[\"\"\"Are there third parties in charge of the license or distribution of  \"\"\"+ context['title']+ \"\"\" dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append( {\n",
    "    \"id\":'metadata.distribution.deprecation_policy', \n",
    "    \"questions\":[\"\"\"Is there any deprecation plan or policy of the \"\"\"+ context['title']+ \"\"\"  dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "## Doing the similarity search\n",
    "for dslConcept in concepts:\n",
    "    ## We perform for each question a semantic similarity\n",
    "    docs = docsearch.similarity_search(dslConcept[\"questions\"][0], k=retrieved_docs)\n",
    "    ## Selecting prompting strategy\n",
    "    if (dslConcept[\"promptStrategy\"] == \"simple\"):\n",
    "        result = incontext_prompt({\"input_documents\": docs, \"question\": dslConcept[\"questions\"][0]},return_only_outputs=True)\n",
    "    elif dslConcept[\"promptStrategy\"] == \"reduce\": \n",
    "        print(\"reduce\")  \n",
    "        result = chain_reduce({\"input_documents\": docs, \"question\": dslConcept[\"questions\"][0]}, return_only_outputs=True)\n",
    "   \n",
    "    specificPart = {\n",
    "    \"id\": dslConcept['id'], \n",
    "    \"questions\":[dslConcept[\"questions\"][0]],\n",
    "    \"promptStrategy\": \"simple\",\n",
    "    \"result\": result['output_text']\n",
    "    }\n",
    "    results.append(specificPart)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e24117",
   "metadata": {},
   "source": [
    "## Composition\n",
    "\n",
    "In that part we get a general rationale using the reduce methods (it is an exaplanation that can be sparse over the document). From this explanations we try to infer the file distribution, and description of each files (parsing the answer). Going depper is difficult, and is information that can be extracted from analyzing the data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7d9d32ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "concepts = []\n",
    "## Distribution section\n",
    "concepts.append({\n",
    "    \"id\":'composition.rationale', \n",
    "    \"questions\":[\"\"\"Which is the format os each file of the dataset?\"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'composition.instances_files', \n",
    "    \"questions\":[\"\"\"Can you enumerate the different files the dataset composed of?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'composition.instances_files.description', \n",
    "    \"questions\":[\"\"\"Can you provide a description of each files the dataset is composed of?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'composition.instances_files.attributes', \n",
    "    \"questions\":[\"\"\"Can you enumerate the different attributes present in the dataset? \n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'composition.instances_files.statistics', \n",
    "    \"questions\":[\"\"\"Are there relevant statistics or distributions of the dataset? \n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "\n",
    "concepts.append( {\n",
    "    \"id\":'composition.consistency_rules', \n",
    "    \"questions\":[\"\"\"Has the data any explicit consistency rule?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append( {\n",
    "    \"id\":'composition.data_splits', \n",
    "    \"questions\":[\"\"\"The paper mentions any recommended data split of the dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "## Doing the similarity search\n",
    "for dslConcept in concepts:\n",
    "    ## We perform for each question a semantic similarity\n",
    "    docs = docsearch.similarity_search(dslConcept[\"questions\"][0], k=retrieved_docs)\n",
    "    ## Selecting prompting strategy\n",
    "    if (dslConcept[\"promptStrategy\"] == \"simple\"):\n",
    "        result = incontext_prompt({\"input_documents\": docs, \"question\": dslConcept[\"questions\"][0]},return_only_outputs=True)\n",
    "    elif dslConcept[\"promptStrategy\"] == \"refine\": \n",
    "        print(\"reduce\")  \n",
    "        result = chain_refine({\"input_documents\": docs, \"question\": dslConcept[\"questions\"][0]}, return_only_outputs=True)\n",
    "    \n",
    "\n",
    "    specificPart = {\n",
    "    \"id\": dslConcept['id'], \n",
    "    \"questions\":[dslConcept[\"questions\"][0]],\n",
    "    \"promptStrategy\": \"simple\",\n",
    "    \"result\": result['output_text']\n",
    "    }\n",
    "    results.append(specificPart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67883a",
   "metadata": {},
   "source": [
    "## Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "7f534722",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Provenance gathering section\n",
    "question = \"\"\"Provide a rationale about how the data of \"\"\"+context['title']+\"\"\" has been collected and prepared. \"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "# result = chain_refine({\"input_documents\": docs, \"question\": question,\"token_max\":1800},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.curation_rationale', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d4531e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Description and Type\n",
    "### Rationale\n",
    "question = \"\"\"Provide a summary of how the data of the dataset has been collected? Please avoid mention the annotation process or data preparation processes\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "### Gathering type\n",
    "question = \"\"\"Which of the following types corresponds to the gathering process mentioned in the context?\n",
    "\n",
    "Types: Web API, Web Scrapping, Sensors, Manual Human Curator, Software collection, Surveys, Observations, Interviews, Focus groups, Document analysis, Secondary data analysis, Physical data collection, Self-reporting, Experiments, Direct measurement, Interviews, Document analysis, Secondary data analysis, Physical data collection, Self-reporting, Experiments, Direct measurement, Customer feedback data, Audio or video recordings, Image data, Biometric data, Medical or health data, Financial data, Geographic or spatial data, Time series data, User-generated content data.\n",
    "\n",
    "Answer with \"Others\", if you are unsure. Please answer with only the type\"\"\"\n",
    "result = incontext_prompt({\"input_documents\": [Document(page_content=result['output_text'],metadata=[])], \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.type', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "9e9a6abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Localization\n",
    "### Gathering Timeframe\n",
    "question = \"Which are the timeframe when the data was collected? \"\n",
    "instruction = \"If present, answer only with the collection timeframe of the data. If your are not sure, or there is no mention, just answers 'not provided'\"\n",
    "\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = chain_instruction_simple({\"instruction\": instruction, \"context\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.timeframe', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "### Gathering geolocalization\n",
    "question = \"\"\"Which are the places where data has been collected?\"\"\"\"\"\n",
    "instruction = \"If present, answer only with the collection timeframe of the data. If your are not sure, or there is no mention, just answers 'not provided'\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = chain_instruction_simple({\"instruction\": instruction, \"context\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.location', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5a869979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "## Sources\n",
    "\n",
    "###  Data Sources\n",
    "question =\"Which is the source of the data during the collection process?\"\n",
    "instruction = \"Answer solely with the name of the source\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.source_description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "###  Infrastructure\n",
    "question =\"Which tools or infrastructure has been used during the collection process?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.source_infra', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e20f046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team Type\n",
    "# Team\n",
    "question = \"Who was the team who collect the data?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.team.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "# Team Type\n",
    "question = \"\"\"The data was collected by an internal team, an external team, or crowdsourcing team?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.team.type', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "question = \"Are the any demographic information of \"+result['output_text']+\"?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.team.demographics', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e961c",
   "metadata": {},
   "source": [
    "## Annotation\n",
    "In that section we use a reduce approach to get a general description of the process, we classifcate it within the different categories using this answer, and then we extract the validations, the demographics and tooling used in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5cd5b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Description and type\n",
    "### description\n",
    "question = \"\"\"How the data of the  \"\"\"+context['title']+\"\"\" has been annotated or labelled? Provide a short summary of the annotation process\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "# type\n",
    "question = \"\"\" Which  of the following category corresponds to the annotation\n",
    "               process mentioned in the context? \n",
    "               \n",
    "            Categories: Bounding boxes, Lines and splines, Semantinc Segmentation, 3D cuboids, Polygonal segmentation, Landmark and key-point, Image and video annotations, Entity annotation, Content and textual categorization\n",
    "               \n",
    "            If you are not sure, answer with 'others'. Please answer only with the categories provided in the context. \"\"\"\n",
    "result = incontext_prompt({\"input_documents\": [Document(page_content=result['output_text'],metadata=[])], \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.type', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"classification\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "\n",
    "# Labels\n",
    "question = \"\"\"\n",
    "Which are the specific labels of the dataset? Can you enumerate it an provide a description of each one?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.labels.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "0f2ccb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team\n",
    "question = \"\"\"Who has annotated the data?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.team.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "# Team Type\n",
    "question = \"\"\"The data was annotated by an internal team, an external team, or crowdsourcing team?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.team.type', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "# Team demographics\n",
    "question = \"\"\"Is there any demographic information about the team who annotate the data?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.team.demographics', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "cf89ff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The dataset was annotated using the NORA image analysis platform, University of Freiburg, Germany.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The labels were validated by an experienced radiologist (S.G., 10 years of experience in hybrid imaging) using dedicated software (NORA image analysis platform, University of Freiburg, Germany). In case of uncertainty regarding lesion definition, the specific PET/CT studies were reviewed in consensus with the radiologist and nuclear medicine physician who prepared the initial clinical report. To this end CT and corresponding PET volumes were displayed side by side or as an overlay and tumor lesions showing elevated FDG-uptake (visually above blood-pool levels) were segmented in a slice-per-slice manner resulting in 3D binary segmentation masks.\n"
     ]
    }
   ],
   "source": [
    "# Infraestructure and Validation\n",
    "question = \"\"\"Which tool has been used to annotate the dataset?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.infrastructure.tool', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "print(result['output_text'])\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "# Validation\n",
    "question = \"\"\"How the quality of the labels have been validated?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.validation.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "print(result['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb48e8",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f5fa0",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Here we use a parsing strategy. In fact we answer for a enumerated list, we parse the list, then we ask for a general description of each process ussing a reduce technique (maybe is explained along the document), and we use this answer as a context for classification task (guessing the type). So, parsing, description and classification. We get the ID as the label. \n",
    "\n",
    "TO DO: If the answer is unknown the process is broken. See how to fix it, by tring to obtain a more constrained structure or putting some \"security\" if the answer is unknow. Such as, \"please asnwer with UKNOWN, if you are not sure\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "56e99c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "\n",
    "## We use the previous answer as a context\n",
    "question_general = \"\"\"Can you enumerate each processes applied to the data to prepare and preprocess the dataset? Avoid answering with the collection process or the annotation process. Plase provide a list of the processes in a short label and comma separated?\n",
    "\n",
    "Example Answer: Data Generation, Data Augmentation, Filtering\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question_general},return_only_outputs=True)\n",
    "\n",
    "parsed = parser.parse(result['output_text'])\n",
    "\n",
    "# For results consistency, ensure that the length of the list is 5, this may be removed for full results\n",
    "while len(parsed) != 5:\n",
    "    if(len(parsed) < 5):\n",
    "        parsed.append(\" \")\n",
    "    if (len(parsed) > 5):\n",
    "        parsed.pop()\n",
    "\n",
    "for parsed_one in parsed:\n",
    "    if (parsed_one == \" \"):\n",
    "        specificPartID = {\n",
    "            \"id\": 'provenance.preprocesses.id', \n",
    "            \"questions\":\"\",\n",
    "            \"promptStrategy\": \"parsing\",\n",
    "            \"result\": \"\"\n",
    "        }\n",
    "        results.append(specificPartID)\n",
    "\n",
    "        specificPartDesc = {\n",
    "        \"id\": 'provenance.preprocesses.description', \n",
    "        \"questions\":\"\",\n",
    "        \"promptStrategy\": \"parsing\",\n",
    "        \"result\": \"\"\n",
    "        }\n",
    "        results.append(specificPartDesc)\n",
    "\n",
    "        specificPartType = {\n",
    "        \"id\": 'provenance.preprocesses.type', \n",
    "        \"questions\":\"\",\n",
    "        \"promptStrategy\": \"simple\",\n",
    "        \"result\": \"\"\n",
    "        }\n",
    "        results.append(specificPartType)\n",
    "    else:\n",
    "        # Description\n",
    "        questionDesc = \"Can you provide a short description of the \"+parsed_one+\" process?\"\n",
    "        docs = docsearch.similarity_search(questionDesc, k=9)\n",
    "        result_description = incontext_prompt({\"input_documents\": docs, \"question\": questionDesc},return_only_outputs=True)\n",
    "\n",
    "    \n",
    "        # Type: We use the previous answer as a contextual info (DOCS)\n",
    "        questionType = \"\"\" Which  of the following category corresponds to\n",
    "                    the \"\"\"+parsed_one+\"\"\" process?\n",
    "                    \n",
    "                    Categories: Missing Values, Data Annotation, Data Augmentation, Outlier Filtering, Remove Duplicates, Data reduction, Sampling, Data Normalization, Others\n",
    "                    \n",
    "                    If you are not sure, answer with 'Others' \"\"\"\n",
    "\n",
    "        result_type = incontext_prompt({\"input_documents\": docs, \"question\": questionType},return_only_outputs=True)\n",
    "\n",
    "        if(result_type['output_text'] != \"Data Annotation\"):\n",
    "            specificPartID = {\n",
    "            \"id\": 'provenance.preprocesses.id', \n",
    "            \"questions\":[question_general],\n",
    "            \"promptStrategy\": \"parsing\",\n",
    "            \"result\": parsed_one\n",
    "            }\n",
    "            results.append(specificPartID)\n",
    "\n",
    "            specificPartDesc = {\n",
    "            \"id\": 'provenance.preprocesses.description', \n",
    "            \"questions\":[questionDesc],\n",
    "            \"promptStrategy\": \"parsing\",\n",
    "            \"result\": result_description['output_text']\n",
    "            }\n",
    "            results.append(specificPartDesc)\n",
    "\n",
    "            specificPartType = {\n",
    "            \"id\": 'provenance.preprocesses.type', \n",
    "            \"questions\":[questionType],\n",
    "            \"promptStrategy\": \"simple\",\n",
    "            \"result\": result_type['output_text']\n",
    "            }\n",
    "            results.append(specificPartType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab67878",
   "metadata": {},
   "source": [
    "## Social Concerns\n",
    "Aquí el que he fet es dividir les preguntes pel tipus de social issue. En aquest cas els autors no classificats els perills del seu dataset en la nostra classificacio. Per tant aquí fem preguntes per temàtica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "dd8be276",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is there any potentia bias in the data?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'social_concerns.bias_social_issue.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "question = \"Are there any social group that could be misrepresented in the dataset?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'social_concerns.representative_social_issue.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "question = \"Are there any imbalance issue  in the dataset?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'social_concerns.imbalance_social_issue.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "question = \"Are there sensitive data, or data that can be offensive for people in the dataset?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'social_concerns.sensitive_social_issue.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "question = \"Is there any privacy issues on the data?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'social_concerns.privacy_social_issue.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d9e2e",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a7dece2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_excel(\"./results/\"+outputfileName+\".xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('enviromentSD': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "138f88d1bae9f8da16f21b0f0d4c90362b2316884551bf1ee4aa819eab74337d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
