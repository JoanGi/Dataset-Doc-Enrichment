{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74148cee",
   "metadata": {},
   "source": [
    "# Code for reproducing the experiment of \"Exploring the use of Language Models to Enhance Datasets Explainability via Documentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00583d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependencies\n",
    "# In order to extract the tables, you need java installed in your system\n",
    "!java -version\n",
    "\n",
    "# Install the requirements.txt of the repo\n",
    "!pip install -r requirements.txt\n",
    "!pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
    "!pip install -q datasets loralib sentencepiece \n",
    "!pip -q install bitsandbytes accelerate\n",
    "!pip -q install langchain\n",
    "!pip -q install tiktoken\n",
    "!pip -q install openai\n",
    "!pip -q install faiss-gpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15764563",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, GenerationConfig, pipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ec43e3",
   "metadata": {},
   "source": [
    "## Configurable parameters\n",
    "The following cell contains the parameters you need to configure in order to use this notebook.\n",
    "In resume: \n",
    "\n",
    "1- You need to point the DocumentPath to a .txt file of the dataset you canto analyze\n",
    "2 - You need to set a name for the Output file via OutputFileName\n",
    "3 - You need to set your API KEY form OpenAI or HF\n",
    "\n",
    "Also:\n",
    "\n",
    "4 - You can selecte the model to use. (Be aware that FLAN-UL2 needs at lest an NVIDIA A100 GPU)\n",
    "\n",
    "Once finished all the cells, you will find a file with the results in the root folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76d6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "## Variables to configure\n",
    "## Select the cleaned dataset file you want to process\n",
    "documentPath = \"sources/Nature-Scientific-Data/A whole-body FDG.txt\"\n",
    "\n",
    "## Set the name of the generated output file with the results (in .xlsx)\n",
    "outputfileName = \"Whole-body Results\"\n",
    "\n",
    "## Table Extraction\n",
    "## Set to 0 if no table extraction is neded\n",
    "## Tables are already extracted in the source files, so keep it to 0.\n",
    "extract_tables = 0\n",
    "## The PDF where the tables are.\n",
    "# pdf_path =\"sources/Nature-Scientific-Data/A whole-body FDG-PET:CT.pdf\"\n",
    "\n",
    "## To adapt the approach to your domain specific use-cases, configure your own semantic dictionary\n",
    "context = {\n",
    "    \"title\": documentPath.split(\"/\")[-1],  \n",
    "    \"gathering\": [\"collection\",\"gathering\", \"acquisition\"],\n",
    "    \"annotation\":[\"labeling\", \"annotation\"],\n",
    "    \"statistics\": [\"Characteristics\", \"Statistics\", \"Features\"],\n",
    "}\n",
    "\n",
    "## NOTE: You will need a APIKEY for OPENAI to use text-davinci-003, or a Hugginface API TOKEN (free) to download the FLAN-UL2\n",
    "api_key=os.getenv(\"OPEN_AI_API_KEY\")\n",
    "api_key_t5=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "## Select the model to use during the experiment\n",
    "## NOTE: Using texct-davinci-003 will not require special hardware requirements. You can execute it in your local setup\n",
    "model = \"text-davinci-003\" \n",
    "## NOTE: Using FLAN-UL2 will require to have at least 40GB of VRAM on your system. \n",
    "## So, FLAN-UL2 will not work on a common local setup. The experiments have beend one using a Nvidia A100 GPU. \n",
    "#model = \"UL2\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e7ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the libraries\n",
    "from langchain.llms import OpenAI, Cohere\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "if model == \"UL2\":\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5ForConditionalGeneration\n",
    "    model_id = 'google/flan-ul2'# go for a smaller model if you dont have the VRAM\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-ul2\")\n",
    "    pipe = pipeline(\n",
    "        model,\n",
    "        tokenizer=tokenizer, \n",
    "        max_length=128,\n",
    "        temperature=0.0,\n",
    "        device_map=\"auto\", \n",
    "        model_kwargs={\"load_in_8bit\": True}\n",
    "    )\n",
    "\n",
    "    LLMClient = HuggingFacePipeline(pipeline=pipe)\n",
    "    retrieved_docs = 10\n",
    "\n",
    "elif model == \"text-davinci-003\":\n",
    "    LLMClient = OpenAI(model_name=model, openai_api_key=api_key,temperature=0)\n",
    "    retrieved_docs = 10\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f0efc",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "First we prepare the data. For this example we do similarity search over a vector database, but these documents could be fetched in any manner (the point of this notebook to highlight what to do AFTER you fetch the documents).\n",
    "### Text and Question preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f28130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8979985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 284, which is longer than the specified 200\n",
      "Created a chunk of size 564, which is longer than the specified 200\n",
      "Created a chunk of size 294, which is longer than the specified 200\n",
      "Created a chunk of size 477, which is longer than the specified 200\n",
      "Created a chunk of size 247, which is longer than the specified 200\n",
      "Created a chunk of size 294, which is longer than the specified 200\n",
      "Created a chunk of size 867, which is longer than the specified 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering\n",
      "gathering: 0\n",
      "collection\n",
      "collection: 2\n",
      "acquisition\n",
      "acquisition: 5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Import the documentation\n",
    "with open(documentPath) as f:\n",
    "    documentation = f.read()\n",
    "\n",
    "# Configure the text splitter and embeddings\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=200, chunk_overlap=10)\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "\n",
    "# Configure semantic dictionary\n",
    "gathering = {\"gathering\": 0}\n",
    "semdict = {\"gathering\": [\"gathering\", \"collection\", \"acquisition\"]}\n",
    "for concept, words in semdict.items():\n",
    "    for word in words:\n",
    "         print(word)\n",
    "         count = sum(1 for match in re.finditer(word, documentation))\n",
    "         print(word+\": \"+ str(count))\n",
    "         if count > gathering[\"gathering\"]:\n",
    "            gathering[\"gathering\"] = count\n",
    "            context['gathering'] = word\n",
    "\n",
    "\n",
    "# Split, encode and index the text\n",
    "texts = text_splitter.split_text(documentation)\n",
    "for idx, text in enumerate(texts):\n",
    "    texts[idx] = text.replace('\\n',' ')\n",
    "    \n",
    "docsearch = FAISS.from_texts(texts, embeddings, metadatas=[{\"source\": i} for i in range(len(texts))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68afef",
   "metadata": {},
   "source": [
    "### Prompt types preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3722373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "## Incontext prompt and refine prompt\n",
    "chain_refine = load_qa_chain(LLMClient, chain_type=\"refine\",return_refine_steps=True)\n",
    "incontext_prompt = load_qa_chain(LLMClient, chain_type=\"stuff\")\n",
    "\n",
    "## Prompt type to reduce the context size, for LLM with lower contexts window.\n",
    "chain_reduce = load_qa_chain(LLMClient, chain_type=\"map_reduce\", return_intermediate_steps=True)\n",
    "\n",
    "## Custom instruction prompt type (Classification and Parsing)\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "{instruction}\n",
    "###\n",
    "Context: \n",
    "{context}\n",
    "###\n",
    "Question: {question}\n",
    "###\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "instruction_prompt = PromptTemplate(\n",
    "    input_variables=[\"instruction\",\"context\",\"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "chain_instruction_simple = LLMChain(llm=LLMClient, prompt=instruction_prompt)\n",
    "\n",
    "## Table prompt to transform parsed tables in natural text\n",
    "prompt_template = \"\"\"Given the following table in HTML, and the given context related the table: Translate the content of the table into natural language.\n",
    "###\n",
    "Context: \n",
    "{context}\n",
    "###\n",
    "Table: {table}\n",
    "###\n",
    "Table translation:\n",
    "\"\"\"\n",
    "table_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\",\"table\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "chain_table = LLMChain(llm=LLMClient, prompt=table_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7644d",
   "metadata": {},
   "source": [
    "### Table Extraction and preparation\n",
    "\n",
    "Tables already extracted for this dataset, so not necessary to execute the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b67c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tables\n",
    "if (extract_tables == 1):\n",
    "    import tabula ## You need to have the Java Tabula installed in the environment\n",
    "    table_texts = []\n",
    "    dfs = tabula.read_pdf(pdf_path, pages='all')\n",
    "    for idx, table in enumerate(dfs):\n",
    "        query = \"Table \"+str(idx+1)+\":\"\n",
    "        docs = docsearch.similarity_search(query, k=4)\n",
    "        result = chain_table({\"context\":docs,\"table\":table})\n",
    "        print(query + \" \"+ result['text'])\n",
    "        table_texts.append(query + \" \"+ result['text'])\n",
    "        # Building the in-context chain with specific instructions\n",
    "    docsearch.add_texts(table_texts,metadatas=[{\"source\": i} for i in range(len(texts))])\n",
    "    with open(documentPath, 'a') as f:\n",
    "        for line in table_texts:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d825b7",
   "metadata": {},
   "source": [
    "# Extraction process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee26e60e",
   "metadata": {},
   "source": [
    "## Uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23dc7ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b927fa94",
   "metadata": {},
   "source": [
    "### Description and Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41177484",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The purpose of the dataset is to provide a publicly available dataset of annotated Positron Emission Tomography/Computed Tomography (PET/CT) studies for deep learning-based automated analysis of PET/CT data.\n"
     ]
    }
   ],
   "source": [
    "# Purposes\n",
    "specificPart = {\n",
    "    \"id\":'metadata.description.purposes', \n",
    "    \"questions\":[\"\"\"Which are the purpose or purposes of the dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"refine\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0], k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0]}, return_only_outputs=True)\n",
    "specificPart['result'] = result['output_text']\n",
    "results.append(specificPart)\n",
    "print(result['output_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ea8b52d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks: Get the the tasks of the dataset with the docs of the purposes. \n",
    "## We get context from previous answer\n",
    "specificPart = {\n",
    "    \"id\":'metadata.description.tasks', \n",
    "    \"questions\":[\"\"\"Which of the following tasks is the dataset inteded for?:\n",
    "\n",
    "            text-classification, question-answering, text-generation, token-classification, translation,\n",
    "            fill-mask, text-retrieval, conditional-text-generation, sequence-modeling, summarization, other,\n",
    "            structure-prediction, information-retrieval, text2text-generation, zero-shot-retrieval,\n",
    "            zero-shot-information-retrieval, automatic-speech-recognition, image-classification, speech-processing,\n",
    "            text-scoring, audio-classification, conversational, question-generation, image-to-text, data-to-text,\n",
    "            classification, object-detection, multiple-choice, text-mining, image-segmentation, dialog-response-generation,\n",
    "            named-entity-recognition, sentiment-analysis, machine-translation, tabular-to-text, table-to-text, simplification,\n",
    "            sentence-similarity, zero-shot-classification, visual-question-answering, text_classification, time-series-forecasting,\n",
    "            computer-vision, feature-extraction, symbolic-regression, topic modeling, one liner summary, email subject, meeting title,\n",
    "            text-to-structured, reasoning, paraphrasing, paraphrase, code-generation, tts, image-retrieval, image-captioning,\n",
    "            language-modelling, video-captionning, neural-machine-translation, transkation, text-generation-other-common-sense-inference,\n",
    "            text-generation-other-discourse-analysis, text-to-tabular, text-generation-other-code-modeling, other-text-search\n",
    "\n",
    "            If you are not sure answer with just with \"others\".\n",
    "            Please, answer only with the one or some of the provided tasks separated by commas. \"\"\"],\n",
    "    \"promptStrategy\": \"classification\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0], k=9)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart['questions'][0]},return_only_outputs=True)\n",
    "specificPart['result'] = result['output_text']\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "58a38d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the tags of the dataset from the doc of purposes\n",
    "## We get context from previous answer\n",
    "specificPart = {\n",
    "    \"id\":'metadata.description.tags', \n",
    "    \"questions\":[\"\"\"Given the context information can you generate a set of representative keywords of it? Please provide the tags comma separated.\"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "}\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart['questions'][0]},return_only_outputs=True)\n",
    "specificPart['result'] = result['output_text']\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "f98c8fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "# Gaps\n",
    "specificPart = {\n",
    "    \"id\":'metadata.description.gaps', \n",
    "    \"questions\":[\"\"\"Which are the gaps the  \"\"\"+ context['title']+ \"\"\" dataset intend to fill?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"reduce\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0], k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0],\"token_max\":1800}, return_only_outputs=True)\n",
    "specificPart['result'] = result['output_text']\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee9055a",
   "metadata": {},
   "source": [
    "### Recommnedations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "928bd105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "## Reccomendations\n",
    "# Recommended\n",
    "question = \"\"\"For which applications the \"\"\"+ context['title']+ \"\"\"   dataset is recommended?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "    \"id\":'metadata.applications.recommended', \n",
    "    \"questions\":[question],\n",
    "    \"promptStrategy\": \"refine\",\n",
    "    \"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "# Non-Recommended\n",
    "question = \"\"\"Is there any non-recommneded application for the \"\"\"+ context['title']+ \"\"\"   dataset? If you are not sure, or there is any non-recommended use of the dataset metioned in the context, just answer with \"no\".\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "    \"id\":'metadata.applications.non_recommended', \n",
    "    \"questions\":[question],\n",
    "    \"promptStrategy\": \"refine\",\n",
    "    \"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa4dff",
   "metadata": {},
   "source": [
    "### ML benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "223a9530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " YES\n",
      " nnUNet13\n",
      "['nnUNet13']\n",
      " nnUNet13:  Dice score, false positive volume, and false negative volume.\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "## Benchmarking\n",
    "question1 = \"\"\"Has the  \"\"\"+ context['title']+ \"\"\"  dataset been tested using any Machine learning technique?\n",
    "                Answer only with a YES or NO\n",
    "                If you are not sure answer with UNSURE\n",
    "                \"\"\"\n",
    "docs = docsearch.similarity_search(question1, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question1},return_only_outputs=True)\n",
    "print(result['output_text'])\n",
    "\n",
    "if \"yes\" in result['output_text'] or \"Yes\" in result['output_text'] or \"YES\" in result['output_text'] :\n",
    "    questionModelNames = \"\"\"Which are the name of the models used to test the dataset? If there is more than one, please provide a list of the models name comma separated\n",
    "                   example answer: Model 1, Model 2, Model 3 \n",
    "                    \n",
    "                    \"\"\"\n",
    "    #docs = docsearch.similarity_search(question2, k=retrieved_docs)\n",
    "    result_sub = incontext_prompt({\"input_documents\": docs, \"question\": questionModelNames},return_only_outputs=True)\n",
    "    models_parsed = parser.parse(result_sub['output_text'])\n",
    "    print (result_sub['output_text'])\n",
    "    print(models_parsed)\n",
    "    result_complete = \"\"\n",
    "    for model in models_parsed: \n",
    "            # Metrics\n",
    "        questionMetrics = \"\"\"Which are the metrics mentioned in the context of the \"\"\"+model+\"\"\" approach?\n",
    "                       If there are no metric just answer with \"no metrics\"\n",
    "                        \"\"\"\n",
    "        # docs = docsearch.similarity_search(questionMetrics, k=retrieved_docs)\n",
    "        result_sub = incontext_prompt({\"input_documents\": docs, \"question\": questionMetrics},return_only_outputs=True)\n",
    "        result_complete = result_complete + \" \" + model +\": \" + result_sub[\"output_text\"]\n",
    "        print(result_complete)\n",
    "\n",
    "    specificPart = {\n",
    "        \"id\":'metadata.applications.benchmarking.modelName', \n",
    "        \"questions\":[question1,questionModelNames],\n",
    "        \"promptStrategy\": \"simple\",\n",
    "        \"result\": result_complete\n",
    "        }\n",
    "    results.append(specificPart)\n",
    "else:\n",
    "    specificPart = {\n",
    "    \"id\":'metadata.applications.benchmarking.modelName', \n",
    "    \"questions\":[question1],\n",
    "    \"promptStrategy\": \"simple\",\n",
    "    \"result\": 'not provided'\n",
    "    }\n",
    "    results.append(specificPart)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8040a",
   "metadata": {},
   "source": [
    "## Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "685e0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Authors\n",
    "# Gaps\n",
    "specificPart = {\n",
    "    \"id\":'metadata.authoring.authors', \n",
    "    \"questions\":[\"\"\"Who are the authors of the \"\"\"+ context['title']+ \"\"\" dataset? Please, answer only with the authors name and affiliation separated by commas.\"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0], k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0]},return_only_outputs=True)\n",
    "specificPart['result'] = result['output_text']\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "6a7b0146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Funders\n",
    "question = \"\"\"Is there any organization which supported or funded the creation of the dataset?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "## Extract the funder's name\n",
    "results.append(\n",
    "    {\n",
    "    \"id\":'metadata.authoring.fundersName', \n",
    "    \"questions\":[\n",
    "        \"\"\"Is there any organization which supported or funded the creation of the dataset?\"\"\"],\n",
    "    \"promptStrategy\": \"in-context\",\n",
    "    \"result\": result['output_text']\n",
    "})\n",
    "## Try to guess funder's type\n",
    "question = \"\"\"The organization mentioned in this context:\n",
    "\n",
    "CONTEXT: \"\"\"+ result['output_text'] + \"\"\"\n",
    "\n",
    "Are of type public, private organizations?\n",
    "If you are not sure answer with just with \"unknown\" \"\"\"\n",
    "result_sub = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "results.append(\n",
    "    {\n",
    "    \"id\":'metadata.authoring.fundersType', \n",
    "    \"questions\":[\n",
    "        \"\"\"The organization mentioned in this context:\n",
    "\n",
    "        CONTEXT: \"\"\"+ result['output_text'] + \"\"\"\n",
    "\n",
    "        Are of public or private organizations?\n",
    "        If you are not sure answer with just with \"unknown\" \"\"\"],\n",
    "    \"promptStrategy\": \"chained\",\n",
    "    \"result\": result_sub['output_text']\n",
    "})\n",
    "\n",
    "\n",
    "## Extracting grantor ID\n",
    "question = \"\"\"Given the context information:\n",
    "\n",
    "    CONTEXT: \"\"\"+ result['output_text'] + \"\"\"\n",
    "\n",
    "    Is there any ID or reference of the grants provided by the funders?\n",
    "    \n",
    "    If you are not sure, answer \"not provided\"\"\"\n",
    "result_sub = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "results.append(\n",
    "    {\n",
    "    \"id\":'metadata.authoring.grantsID', \n",
    "    \"questions\":[\n",
    "        \"\"\"Given the context information:\n",
    "\n",
    "        CONTEXT: \"\"\"+ result['output_text'] + \"\"\"\n",
    "\n",
    "        Which is the ID of the grant?\n",
    "        \n",
    "        If you are not sure, answer \"not provided\"\"\"],\n",
    "    \"promptStrategy\": \"chained\",\n",
    "    \"result\": result_sub['output_text']\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "aed68875",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Maintainers\n",
    "specificPart = {\n",
    "    \"id\":'metadata.authoring.maintainers', \n",
    "    \"questions\":[\"\"\"Who are the maintainers of the  \"\"\"+ context['title']+ \"\"\" dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"reduce\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0], k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0], \"token_max\":1800}, return_only_outputs=True)\n",
    "specificPart[\"result\"] = result['output_text']\n",
    "results.append(specificPart)\n",
    "\n",
    "## Contribution Guidelines\n",
    "specificPart= {\n",
    "    \"id\":'metadata.authoring.contribution_guidelines', \n",
    "    \"questions\":[\"\"\"Which are the contribution guidelines of the  \"\"\"+ context['title']+ \"\"\" dataset? If you are not sure, or there is no contribution guidelines just answer with \"no\".\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0],k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0]},return_only_outputs=True)\n",
    "specificPart[\"result\"] = result['output_text']\n",
    "results.append(specificPart)\n",
    "\n",
    "# Erratum\n",
    "specificPart = {\n",
    "    \"id\":'metadata.authoring.erratum', \n",
    "    \"questions\":[\"\"\"Is there any data retention limit in the  \"\"\"+ context['title']+ \"\"\" dataset? If you are not sure, or there is no retention limit just answer with \"no\".\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0], k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0]},return_only_outputs=True)\n",
    "specificPart[\"result\"] = result['output_text']\n",
    "results.append(specificPart)\n",
    "\n",
    "# Retention\n",
    "specificPart = {\n",
    "    \"id\":'metadata.authoring.data_retention', \n",
    "    \"questions\":[\"\"\"Is there any data retention policies policiy of the  \"\"\"+ context['title']+ \"\"\" dataset? If you are not sure, or there is no retention policy just answer with \"no\".\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "}\n",
    "docs = docsearch.similarity_search(specificPart[\"questions\"][0])\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": specificPart[\"questions\"][0]},return_only_outputs=True)\n",
    "specificPart[\"result\"] = result['output_text']\n",
    "results.append(specificPart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8084fa",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "70151192",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = []\n",
    "## Distribution section\n",
    "concepts.append({\n",
    "    \"id\":'metadata.distribution.data_repository', \n",
    "    \"questions\":[\"\"\"Is there a link to the a repository containing the data? If you are not sure, or there is no link to the repository just answer with \"no\".\"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'metadata.distribution.license', \n",
    "    \"questions\":[\"\"\"Which is the license of the  \"\"\" +context['title']+\"\"\"  dataset. If you are not sure, or there is mention to a license of the dataset in the context, just answer with \"no\".\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'metadata.distribution.rights_of_data', \n",
    "    \"questions\":[\"\"\"Which are the rights of the stand-alone dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'metadata.distribution.rights_of_model', \n",
    "    \"questions\":[\"\"\"Which are the rights of the models trained with this data?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'metadata.distribution.attribution_credits', \n",
    "    \"questions\":[\"\"\"Is there any attribution notice that have to be used to use the {  \"\"\"+ context['title']+ \"\"\" dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append( {\n",
    "    \"id\":'metadata.distribution.designated_third_parties', \n",
    "    \"questions\":[\"\"\"Are there third parties in charge of the license or distribution of  \"\"\"+ context['title']+ \"\"\" dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append( {\n",
    "    \"id\":'metadata.distribution.deprecation_policy', \n",
    "    \"questions\":[\"\"\"Is there any deprecation plan or policy of the \"\"\"+ context['title']+ \"\"\"  dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "## Doing the similarity search\n",
    "for dslConcept in concepts:\n",
    "    ## We perform for each question a semantic similarity\n",
    "    docs = docsearch.similarity_search(dslConcept[\"questions\"][0], k=retrieved_docs)\n",
    "    ## Selecting prompting strategy\n",
    "    if (dslConcept[\"promptStrategy\"] == \"simple\"):\n",
    "        result = incontext_prompt({\"input_documents\": docs, \"question\": dslConcept[\"questions\"][0]},return_only_outputs=True)\n",
    "    elif dslConcept[\"promptStrategy\"] == \"reduce\": \n",
    "        print(\"reduce\")  \n",
    "        result = chain_reduce({\"input_documents\": docs, \"question\": dslConcept[\"questions\"][0]}, return_only_outputs=True)\n",
    "   \n",
    "    specificPart = {\n",
    "    \"id\": dslConcept['id'], \n",
    "    \"questions\":[dslConcept[\"questions\"][0]],\n",
    "    \"promptStrategy\": \"simple\",\n",
    "    \"result\": result['output_text']\n",
    "    }\n",
    "    results.append(specificPart)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e24117",
   "metadata": {},
   "source": [
    "## Composition\n",
    "\n",
    "In that part we get a general rationale using the reduce methods (it is an exaplanation that can be sparse over the document). From this explanations we try to infer the file distribution, and description of each files (parsing the answer). Going depper is difficult, and is information that can be extracted from analyzing the data directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7d9d32ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "concepts = []\n",
    "## Distribution section\n",
    "concepts.append({\n",
    "    \"id\":'composition.rationale', \n",
    "    \"questions\":[\"\"\"Which is the format os each file of the dataset?\"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'composition.instances_files', \n",
    "    \"questions\":[\"\"\"Can you enumerate the different files the dataset composed of?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'composition.instances_files.description', \n",
    "    \"questions\":[\"\"\"Can you provide a description of each files the dataset is composed of?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'composition.instances_files.attributes', \n",
    "    \"questions\":[\"\"\"Can you enumerate the different attributes present in the dataset? \n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append({\n",
    "    \"id\":'composition.instances_files.statistics', \n",
    "    \"questions\":[\"\"\"Are there relevant statistics or distributions of the dataset? \n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "\n",
    "concepts.append( {\n",
    "    \"id\":'composition.consistency_rules', \n",
    "    \"questions\":[\"\"\"Has the data any explicit consistency rule?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "concepts.append( {\n",
    "    \"id\":'composition.data_splits', \n",
    "    \"questions\":[\"\"\"The paper mentions any recommended data split of the dataset?\n",
    "                \"\"\"],\n",
    "    \"promptStrategy\": \"simple\"\n",
    "})\n",
    "## Doing the similarity search\n",
    "for dslConcept in concepts:\n",
    "    ## We perform for each question a semantic similarity\n",
    "    docs = docsearch.similarity_search(dslConcept[\"questions\"][0], k=retrieved_docs)\n",
    "    ## Selecting prompting strategy\n",
    "    if (dslConcept[\"promptStrategy\"] == \"simple\"):\n",
    "        result = incontext_prompt({\"input_documents\": docs, \"question\": dslConcept[\"questions\"][0]},return_only_outputs=True)\n",
    "    elif dslConcept[\"promptStrategy\"] == \"refine\": \n",
    "        print(\"reduce\")  \n",
    "        result = chain_refine({\"input_documents\": docs, \"question\": dslConcept[\"questions\"][0]}, return_only_outputs=True)\n",
    "    \n",
    "\n",
    "    specificPart = {\n",
    "    \"id\": dslConcept['id'], \n",
    "    \"questions\":[dslConcept[\"questions\"][0]],\n",
    "    \"promptStrategy\": \"simple\",\n",
    "    \"result\": result['output_text']\n",
    "    }\n",
    "    results.append(specificPart)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b67883a",
   "metadata": {},
   "source": [
    "## Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "7f534722",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Provenance gathering section\n",
    "question = \"\"\"Provide a rationale about how the data of \"\"\"+context['title']+\"\"\" has been collected and prepared. \"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "# result = chain_refine({\"input_documents\": docs, \"question\": question,\"token_max\":1800},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.curation_rationale', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "d4531e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Description and Type\n",
    "### Rationale\n",
    "question = \"\"\"Provide a summary of how the data of the dataset has been collected? Please avoid mention the annotation process or data preparation processes\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "### Gathering type\n",
    "question = \"\"\"Which of the following types corresponds to the gathering process mentioned in the context?\n",
    "\n",
    "Types: Web API, Web Scrapping, Sensors, Manual Human Curator, Software collection, Surveys, Observations, Interviews, Focus groups, Document analysis, Secondary data analysis, Physical data collection, Self-reporting, Experiments, Direct measurement, Interviews, Document analysis, Secondary data analysis, Physical data collection, Self-reporting, Experiments, Direct measurement, Customer feedback data, Audio or video recordings, Image data, Biometric data, Medical or health data, Financial data, Geographic or spatial data, Time series data, User-generated content data.\n",
    "\n",
    "Answer with \"Others\", if you are unsure. Please answer with only the type\"\"\"\n",
    "result = incontext_prompt({\"input_documents\": [Document(page_content=result['output_text'],metadata=[])], \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.type', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "9e9a6abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Localization\n",
    "### Gathering Timeframe\n",
    "question = \"Which are the timeframe when the data was collected? \"\n",
    "instruction = \"If present, answer only with the collection timeframe of the data. If your are not sure, or there is no mention, just answers 'not provided'\"\n",
    "\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = chain_instruction_simple({\"instruction\": instruction, \"context\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.timeframe', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "### Gathering geolocalization\n",
    "question = \"\"\"Which are the places where data has been collected?\"\"\"\"\"\n",
    "instruction = \"If present, answer only with the collection timeframe of the data. If your are not sure, or there is no mention, just answers 'not provided'\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = chain_instruction_simple({\"instruction\": instruction, \"context\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.location', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5a869979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "## Sources\n",
    "\n",
    "###  Data Sources\n",
    "question =\"Which is the source of the data during the collection process?\"\n",
    "instruction = \"Answer solely with the name of the source\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.source_description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "###  Infrastructure\n",
    "question =\"Which tools or infrastructure has been used during the collection process?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.source_infra', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "e20f046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team Type\n",
    "# Team\n",
    "question = \"Who was the team who collect the data?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.team.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "# Team Type\n",
    "question = \"\"\"The data was collected by an internal team, an external team, or crowdsourcing team?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.team.type', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "question = \"Are the any demographic information of \"+result['output_text']+\"?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.gathering.team.demographics', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e961c",
   "metadata": {},
   "source": [
    "## Annotation\n",
    "In that section we use a reduce approach to get a general description of the process, we classifcate it within the different categories using this answer, and then we extract the validations, the demographics and tooling used in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5cd5b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Description and type\n",
    "### description\n",
    "question = \"\"\"How the data of the  \"\"\"+context['title']+\"\"\" has been annotated or labelled? Provide a short summary of the annotation process\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "# type\n",
    "question = \"\"\" Which  of the following category corresponds to the annotation\n",
    "               process mentioned in the context? \n",
    "               \n",
    "            Categories: Bounding boxes, Lines and splines, Semantinc Segmentation, 3D cuboids, Polygonal segmentation, Landmark and key-point, Image and video annotations, Entity annotation, Content and textual categorization\n",
    "               \n",
    "            If you are not sure, answer with 'others'. Please answer only with the categories provided in the context. \"\"\"\n",
    "result = incontext_prompt({\"input_documents\": [Document(page_content=result['output_text'],metadata=[])], \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.type', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"classification\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "\n",
    "# Labels\n",
    "question = \"\"\"\n",
    "Which are the specific labels of the dataset? Can you enumerate it an provide a description of each one?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.labels.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "0f2ccb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Team\n",
    "question = \"\"\"Who has annotated the data?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.team.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "# Team Type\n",
    "question = \"\"\"The data was annotated by an internal team, an external team, or crowdsourcing team?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.team.type', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "# Team demographics\n",
    "question = \"\"\"Is there any demographic information about the team who annotate the data?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.team.demographics', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "cf89ff93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The dataset was annotated using the NORA image analysis platform, University of Freiburg, Germany.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The labels were validated by an experienced radiologist (S.G., 10 years of experience in hybrid imaging) using dedicated software (NORA image analysis platform, University of Freiburg, Germany). In case of uncertainty regarding lesion definition, the specific PET/CT studies were reviewed in consensus with the radiologist and nuclear medicine physician who prepared the initial clinical report. To this end CT and corresponding PET volumes were displayed side by side or as an overlay and tumor lesions showing elevated FDG-uptake (visually above blood-pool levels) were segmented in a slice-per-slice manner resulting in 3D binary segmentation masks.\n"
     ]
    }
   ],
   "source": [
    "# Infraestructure and Validation\n",
    "question = \"\"\"Which tool has been used to annotate the dataset?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.infrastructure.tool', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "print(result['output_text'])\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "# Validation\n",
    "question = \"\"\"How the quality of the labels have been validated?\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'provenance.labeling.validation.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "print(result['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb48e8",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f5fa0",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Here we use a parsing strategy. In fact we answer for a enumerated list, we parse the list, then we ask for a general description of each process ussing a reduce technique (maybe is explained along the document), and we use this answer as a context for classification task (guessing the type). So, parsing, description and classification. We get the ID as the label. \n",
    "\n",
    "TO DO: If the answer is unknown the process is broken. See how to fix it, by tring to obtain a more constrained structure or putting some \"security\" if the answer is unknow. Such as, \"please asnwer with UKNOWN, if you are not sure\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "56e99c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "\n",
    "## We use the previous answer as a context\n",
    "question_general = \"\"\"Can you enumerate each processes applied to the data to prepare and preprocess the dataset? Avoid answering with the collection process or the annotation process. Plase provide a list of the processes in a short label and comma separated?\n",
    "\n",
    "Example Answer: Data Generation, Data Augmentation, Filtering\"\"\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question_general},return_only_outputs=True)\n",
    "\n",
    "parsed = parser.parse(result['output_text'])\n",
    "\n",
    "# For results consistency, ensure that the length of the list is 5, this may be removed for full results\n",
    "while len(parsed) != 5:\n",
    "    if(len(parsed) < 5):\n",
    "        parsed.append(\" \")\n",
    "    if (len(parsed) > 5):\n",
    "        parsed.pop()\n",
    "\n",
    "for parsed_one in parsed:\n",
    "    if (parsed_one == \" \"):\n",
    "        specificPartID = {\n",
    "            \"id\": 'provenance.preprocesses.id', \n",
    "            \"questions\":\"\",\n",
    "            \"promptStrategy\": \"parsing\",\n",
    "            \"result\": \"\"\n",
    "        }\n",
    "        results.append(specificPartID)\n",
    "\n",
    "        specificPartDesc = {\n",
    "        \"id\": 'provenance.preprocesses.description', \n",
    "        \"questions\":\"\",\n",
    "        \"promptStrategy\": \"parsing\",\n",
    "        \"result\": \"\"\n",
    "        }\n",
    "        results.append(specificPartDesc)\n",
    "\n",
    "        specificPartType = {\n",
    "        \"id\": 'provenance.preprocesses.type', \n",
    "        \"questions\":\"\",\n",
    "        \"promptStrategy\": \"simple\",\n",
    "        \"result\": \"\"\n",
    "        }\n",
    "        results.append(specificPartType)\n",
    "    else:\n",
    "        # Description\n",
    "        questionDesc = \"Can you provide a short description of the \"+parsed_one+\" process?\"\n",
    "        docs = docsearch.similarity_search(questionDesc, k=9)\n",
    "        result_description = incontext_prompt({\"input_documents\": docs, \"question\": questionDesc},return_only_outputs=True)\n",
    "\n",
    "    \n",
    "        # Type: We use the previous answer as a contextual info (DOCS)\n",
    "        questionType = \"\"\" Which  of the following category corresponds to\n",
    "                    the \"\"\"+parsed_one+\"\"\" process?\n",
    "                    \n",
    "                    Categories: Missing Values, Data Annotation, Data Augmentation, Outlier Filtering, Remove Duplicates, Data reduction, Sampling, Data Normalization, Others\n",
    "                    \n",
    "                    If you are not sure, answer with 'Others' \"\"\"\n",
    "\n",
    "        result_type = incontext_prompt({\"input_documents\": docs, \"question\": questionType},return_only_outputs=True)\n",
    "\n",
    "        if(result_type['output_text'] != \"Data Annotation\"):\n",
    "            specificPartID = {\n",
    "            \"id\": 'provenance.preprocesses.id', \n",
    "            \"questions\":[question_general],\n",
    "            \"promptStrategy\": \"parsing\",\n",
    "            \"result\": parsed_one\n",
    "            }\n",
    "            results.append(specificPartID)\n",
    "\n",
    "            specificPartDesc = {\n",
    "            \"id\": 'provenance.preprocesses.description', \n",
    "            \"questions\":[questionDesc],\n",
    "            \"promptStrategy\": \"parsing\",\n",
    "            \"result\": result_description['output_text']\n",
    "            }\n",
    "            results.append(specificPartDesc)\n",
    "\n",
    "            specificPartType = {\n",
    "            \"id\": 'provenance.preprocesses.type', \n",
    "            \"questions\":[questionType],\n",
    "            \"promptStrategy\": \"simple\",\n",
    "            \"result\": result_type['output_text']\n",
    "            }\n",
    "            results.append(specificPartType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab67878",
   "metadata": {},
   "source": [
    "## Social Concerns\n",
    "Aquí el que he fet es dividir les preguntes pel tipus de social issue. En aquest cas els autors no classificats els perills del seu dataset en la nostra classificacio. Per tant aquí fem preguntes per temàtica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "dd8be276",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is there any potentia bias in the data?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "specificPart = {\n",
    "\"id\": 'social_concerns.bias_social_issue.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "question = \"Are there any social group that could be misrepresented in the dataset?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'social_concerns.representative_social_issue.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "question = \"Are there any imbalance issue  in the dataset?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'social_concerns.imbalance_social_issue.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "question = \"Are there sensitive data, or data that can be offensive for people in the dataset?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'social_concerns.sensitive_social_issue.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)\n",
    "\n",
    "\n",
    "question = \"Is there any privacy issues on the data?\"\n",
    "docs = docsearch.similarity_search(question, k=retrieved_docs)\n",
    "result = incontext_prompt({\"input_documents\": docs, \"question\": question},return_only_outputs=True)\n",
    "\n",
    "specificPart = {\n",
    "\"id\": 'social_concerns.privacy_social_issue.description', \n",
    "\"questions\":[question],\n",
    "\"promptStrategy\": \"simple\",\n",
    "\"result\": result['output_text']\n",
    "}\n",
    "results.append(specificPart)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d9e2e",
   "metadata": {},
   "source": [
    "# Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "a7dece2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df.to_excel(\"./results/\"+outputfileName+\".xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('enviromentSD': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "138f88d1bae9f8da16f21b0f0d4c90362b2316884551bf1ee4aa819eab74337d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
